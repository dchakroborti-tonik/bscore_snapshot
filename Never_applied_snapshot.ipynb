{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9c3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858c7c8",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632a8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b5f2a",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fba9c1",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707a20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75766b0a",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3b76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751a30f",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac61d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eb04f",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080a4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8776f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1d410",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20c39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = 'risk_mart'\n",
    "\n",
    "\n",
    "al = f'applied_loans_20230101_{CURRENT_DATE}'\n",
    "altrans = f'applied_loans_20210701_{CURRENT_DATE}_trans'\n",
    "nal = f'tsa_onboarded_but_never_applied_loan_20230101_{CURRENT_DATE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b98dd",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.trench2_never_applied_snapshot_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca47b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "create or replace table `prj-prod-dataplatform.worktable_data_analysis.trench2_never_applied_snapshot_transactions` as \n",
    "with input_customers as (\n",
    "SELECT \n",
    "customerId,\n",
    "credolabRefNumber onb_credolabRefNumber,\n",
    "credo_inquiry_date as onb_credo_inquiry_date,\n",
    "onb_tsa_onboarding_datetime,\n",
    "onb_age,\n",
    "onb_doc_type,\n",
    "onb_os_version as onb_osversion,\n",
    "onb_email,\n",
    "ln_industry_new,\n",
    "ln_employment_type_new,\n",
    "DATE('2025-08-10') as snapshot_date,\n",
    "DATE_DIFF(DATE('2025-08-10'),DATE(onb_tsa_onboarding_datetime),DAY) dob_snapshot_date,\n",
    "DATE_DIFF(DATE(onb_tsa_onboarding_datetime),credo_inquiry_date,DAY) days_since_credo_call_onb,\n",
    "DATE_DIFF(DATE('2025-08-10'),credo_inquiry_date,DAY) AS days_since_credo_call_snapshot_date,\n",
    "ln_self_dec_income,\n",
    "ln_marital_status,\n",
    "ln_education_level,\n",
    "ln_nature_of_work_new,\n",
    "ln_source_funds_new,\n",
    "onb_first_name,\n",
    "onb_middle_name,\n",
    "onb_city,\n",
    "onb_province,\n",
    "onb_last_name,\n",
    "ln_brand,\n",
    "ln_cnt_dependents,\n",
    "ln_doc_type,\n",
    "ln_osversion,\n",
    "onb_kyc_status,\n",
    "onb_latitude,\n",
    "onb_longitude,\n",
    "ln_user_type,\n",
    "ln_loan_type,\n",
    "ln_prod_type,\n",
    "ln_loan_applied_flag,\n",
    "c_credo_score,\n",
    "cust_status_flag,\n",
    "cust_status_close_date,\n",
    "\n",
    "FROM\n",
    "`risk_mart.tsa_onboarded_but_never_applied_loan_20230101_20250810`\n",
    "--where DATE_ADD(onb_tsa_onboarding_datetime,INTERVAL 180 DAY) <= '2025-03-09'\n",
    "),\n",
    "cust_onboarding_acc_data as (\n",
    "    SELECT\n",
    "    DATE(opendate,'Asia/Manila') OFDATEOPENED,\n",
    "    DATE(closuredate,'Asia/Manila') ofdateclosed,\n",
    "    closed OFISCLOSED,\n",
    "    DATE(c.created_dt) registration_date,\n",
    "    c.created_dt as onboarding_date,\n",
    "    cust_id,\n",
    "    DATETIME(reccreatedon,'Asia/Manila') reccreatedon,\n",
    "    accountid,\n",
    "    productid,\n",
    "    accountdescription as account_type,\n",
    "    clearedbalance,\n",
    "    snapshot_date\n",
    "    FROM `dl_customers_db_raw.tdbk_customer_mtb` c    \n",
    "    JOIN `finastra_raw.account` b ON c.cust_id = b.ubcustomercode\n",
    "    JOIN input_customers on c.cust_id = CAST(input_customers.customerid AS STRING)\n",
    "),\n",
    "main_transaction_data AS \n",
    "(\n",
    "     SELECT \n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    b.cust_id customer_id,\n",
    "    a.accountid,\n",
    "    productid,\n",
    "    b.account_type,\n",
    "    transaction_code,\n",
    "    a.status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    input_customers.snapshot_date\n",
    "    -- customer_transactions to get the transactions\n",
    "    FROM cust_onboarding_acc_data b\n",
    "    JOIN input_customers on b.cust_id = CAST(input_customers.customerid AS STRING)\n",
    "    LEFT JOIN `risk_mart.customer_transactions` a ON a.accountid = b.accountid\n",
    "    and a.transaction_date < input_customers.snapshot_date and a.status = 'Success'\n",
    "    --and a.transaction_datetime < input_customers.snapshot_date\n",
    "    WHERE 1=1\n",
    "   \n",
    "),\n",
    "\n",
    "\n",
    "#### Net Cash In ####\n",
    "  -- 1. Outside Tonik to TSA\n",
    "  -- 2. Other Tonik user to Own Tonik Account\n",
    "\n",
    "\n",
    "net_cash_in AS \n",
    "(\n",
    "  ## 1. Outside Tonik to TSA\n",
    "  SELECT\n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash In' main_transaction_type,\n",
    "    'Outside Tonik to TSA' sub_transaction_type,\n",
    "    snapshot_date\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and credit and all coming from Tonik Account\n",
    "  AND credit_debit_indicator = 'CREDIT'\n",
    "  AND account_type = 'Tonik Account' and LOWER(core_narration) NOT LIKE '%blocking%' and transaction_code not like 'A0%'\n",
    "  AND transaction_code IN ('N01','IP2','XE2','00T','21C','P01')\n",
    "  -- 1. Outside Tonik to TSA conditions (all cash in)\n",
    "  AND inter_exter_flag = 'Outside Tonik'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 2. Other Tonik user to Own Tonik Account\n",
    "  SELECT\n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash In' main_transaction_type,\n",
    "    'Other Tonik Users to Town Tonik Account' sub_transaction_type,\n",
    "    snapshot_date\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and credit and all coming from Tonik Account\n",
    "  AND credit_debit_indicator = 'CREDIT'\n",
    "  AND account_type = 'Tonik Account' and LOWER(core_narration) NOT LIKE '%blocking%' and transaction_code not like 'A0%'\n",
    "  AND transaction_code IN ('N01','IP2','XE2','00T','21C','P01')\n",
    "\n",
    "  -- 2. Other Tonik user to Own Tonik Account\n",
    "  AND inter_exter_flag = 'Inside Tonik'\n",
    "  AND core_narration LIKE '%Receive money from other Tonik Account%'\n",
    "  -- AND LEFT(core_narration,STRPOS(core_narration, \",\")-1) = 'Receive money from other Tonik Account'\n",
    ")\n",
    "\n",
    "#### Net Cash Out ####\n",
    "-- 1. Bills Pay\n",
    "-- 2. Card Transactions\n",
    "-- 3. Own TSA to other Tonik Users\n",
    "-- 4. TSA to Outside Tonik\n",
    "\n",
    ", net_cash_out AS \n",
    "(\n",
    "\n",
    " ## 1. Bills Pay\n",
    "  SELECT \n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Bills Pay' sub_transaction_type,\n",
    "    snapshot_date,\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND credit_debit_indicator = 'DEBIT'\n",
    "  AND LOWER(core_narration) NOT LIKE '%blocking%'\n",
    "\n",
    "  -- 1. Bills Pay\n",
    "  AND channel = 'Billspay'\n",
    "\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 2. Card Transactions (Cash Out)\n",
    "  SELECT\n",
    "    a.transaction_date,\n",
    "    a.OFDATEOPENED,\n",
    "    a.OFISCLOSED,\n",
    "    a.registration_date,\n",
    "    a.transaction_id,\n",
    "    a.customer_id,\n",
    "    a.accountid,\n",
    "    a.account_type,\n",
    "    a.status,\n",
    "    a.channel,\n",
    "    a.credit_debit_indicator,\n",
    "    a.inter_exter_flag,\n",
    "    a.trx_amount,\n",
    "    a.core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Card Transactions (Cash Out)' sub_transaction_type,\n",
    "    snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  -- 2. Card Transactions (Cash Out) -- using the table made above\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit and coming from tonik account\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND transaction_code like 'A0%' and core_narration not like '%Blocking%'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 3. Own TSA to other Tonik Users\n",
    "  SELECT DISTINCT\n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Own TSA to Other Tonik Users' sub_transaction_type,\n",
    "    snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND transaction_code not like 'A0%' and core_narration not like '%Blocking%'\n",
    "\n",
    "  -- 3. Own TSA to other Tonik Users\n",
    "  AND a.channel = 'Core transactions'\n",
    "  AND a.inter_exter_flag = 'Inside Tonik'\n",
    "  --AND LOWER(core_narration) LIKE '%send money to other tonik account%' \n",
    "  -- AND LOWER(core_narration) NOT LIKE '%scontri%'\n",
    "  -- AND LOWER(core_narration) NOT LIKE '%stash%'\n",
    "  -- AND LOWER(core_narration) NOT LIKE '%time deposit%'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 4. TSA to Outside Tonik (Other banks)\n",
    "  SELECT DISTINCT\n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'TSA to Outside Tonik (Other Banks)' sub_transaction_type,\n",
    "    snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND core_narration not like '%Blocking%'\n",
    "\n",
    "  -- channels not in core transactions and billspay with the flag as outside tonik are sending to other banks\n",
    "  AND a.channel NOT IN  ('Core transactions','Billspay')\n",
    "  AND a.inter_exter_flag = 'Outside Tonik'\n",
    ")\n",
    "\n",
    ", transactions_sub AS \n",
    "(\n",
    "  -- merging the cash ins and cash outs\n",
    "  SELECT DISTINCT *\n",
    "  FROM net_cash_in \n",
    "  UNION ALL\n",
    "  SELECT DISTINCT *\n",
    "  FROM net_cash_out\n",
    ")\n",
    "\n",
    ", date_diff_sub AS \n",
    "(\n",
    "    -- to get the date difference between 2 transactions (cash in and cash out)\n",
    "    SELECT customer_id,\n",
    "    'Overall' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date,core_narration ASC),transaction_date,DAY) days_bt_trans,\n",
    "    snapshot_date\n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE 1=1\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "    )\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- to get the date difference between 2 cash ins\n",
    "    SELECT customer_id,\n",
    "    'Cash In' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date ASC),transaction_date,DAY) days_bt_trans,\n",
    "    snapshot_date\n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE 1=1\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "        AND main_transaction_type = 'Net Cash In'\n",
    "    )\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- to get the date difference between 2 cash outs\n",
    "    SELECT customer_id,\n",
    "    'Cash Out' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date,core_narration ASC),transaction_date,DAY) days_bt_trans,\n",
    "    snapshot_date\n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE 1=1\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "        AND main_transaction_type = 'Net Cash Out'\n",
    "    )\n",
    ")\n",
    "\n",
    ", days_bt_trans_avg AS \n",
    "(\n",
    "-- get the average days in between \n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "AVG(IF(days_diff_type='Overall',days_bt_trans,NULL)) overall_avg_days_bt_trans,\n",
    "AVG(IF(days_diff_type='Cash In',days_bt_trans,NULL)) net_cash_in_avg_days_bt_trans,\n",
    "AVG(IF(days_diff_type='Cash Out',days_bt_trans,NULL)) net_cash_out_avg_days_bt_trans\n",
    "FROM date_diff_sub\n",
    "GROUP BY 1\n",
    ")\n",
    "\n",
    ", days_bt_trans_med AS \n",
    "(\n",
    "-- get the median days in between\n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Overall',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) overall_med_days_bt_trans,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Cash In',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) cash_in_med_days_bt_trans,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Cash Out',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) cash_out_med_days_bt_trans,\n",
    "FROM date_diff_sub\n",
    ")\n",
    "\n",
    ", transactions_final AS \n",
    "(\n",
    "SELECT DISTINCT\n",
    "acc.customerid as customer_id,\n",
    "## Number of transactions within the observation window (x days from onboarding date),\n",
    "## Cash In Count Details\n",
    "COUNT(DISTINCT(IF(main_transaction_type = 'Net Cash In',transaction_id,NULL))) tx_cnt_cash_in_total,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type='Outside Tonik to TSA' ,transaction_id,NULL))) tx_cnt_cash_in_ob2t,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type='Other Tonik Users to Town Tonik Account' ,transaction_id,NULL))) tx_cnt_cash_in_ot2t,\n",
    "\n",
    "## Cash In Amount Details\n",
    "SUM(IF(main_transaction_type = 'Net Cash In',trx_amount,NULL)) tx_amt_cash_in_total,\n",
    "SUM((IF(sub_transaction_type='Outside Tonik to TSA' ,trx_amount,NULL))) tx_amt_cash_in_ob2t,\n",
    "SUM((IF(sub_transaction_type='Other Tonik Users to Town Tonik Account',trx_amount,NULL))) tx_amt_cash_in_ot2t,\n",
    "\n",
    "## Cash Out Count Details\n",
    "COUNT(DISTINCT(IF(main_transaction_type = 'Net Cash Out',transaction_id,NULL))) tx_cnt_cash_out_total,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Bills Pay' ,transaction_id,NULL))) tx_cnt_cash_out_billpay,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Card Transactions (Cash Out)' ,transaction_id,NULL))) tx_cnt_cash_out_cards,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Own TSA to Other Tonik Users' ,transaction_id,NULL))) tx_cnt_cash_out_t2ot,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'TSA to Outside Tonik (Other Banks)' ,transaction_id,NULL))) tx_cnt_cash_out_t2ob,\n",
    "\n",
    "## Cash Out Amount Details\n",
    "SUM(IF(main_transaction_type = 'Net Cash Out',trx_amount,NULL)) tx_amt_cash_out_total,\n",
    "SUM(IF(sub_transaction_type= 'Bills Pay' ,trx_amount,NULL)) tx_amt_cash_out_billpay,\n",
    "SUM(IF(sub_transaction_type= 'Card Transactions (Cash Out)' ,trx_amount,NULL)) tx_amt_cash_out_cards,\n",
    "SUM(IF(sub_transaction_type= 'Own TSA to Other Tonik Users' ,trx_amount,NULL)) tx_amt_cash_out_t2ot,\n",
    "SUM(IF(sub_transaction_type= 'TSA to Outside Tonik (Other Banks)' ,trx_amount,NULL)) tx_amt_cash_out_t2ob,\n",
    "FROM input_customers acc \n",
    "LEFT JOIN transactions_sub a ON CAST(acc.customerid AS STRING) = a.customer_id and date(transaction_date) < date(acc.snapshot_date)\n",
    "GROUP BY 1\n",
    "ORDER BY 2 \n",
    "\n",
    "),\n",
    "\n",
    "utility_transaction_data AS (\n",
    "        SELECT \n",
    "        customer_id,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code IN ('BP1', 'BP2', 'BP3', 'BP4') THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS first_billpay_date,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code like 'A0%' AND core_narration NOT LIKE '%Blocking%' THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS virtual_transaction_date,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code IN ('21C', 'N01', 'IP2', 'XE2','P01') THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS first_tsa_topup_date\n",
    "    FROM \n",
    "        main_transaction_data\n",
    "    GROUP BY 1\n",
    "),\n",
    "\n",
    "combined_data AS (\n",
    "    SELECT \n",
    "        COALESCE(acc.cust_id,acc_data.customer_id,utility_transaction_data.customer_id) customer_id,\n",
    "        productid,\n",
    "        accountdescription,\n",
    "        acc_data.opendate as opendate ,\n",
    "        first_billpay_date,\n",
    "        virtual_transaction_date,\n",
    "        first_tsa_topup_date,\n",
    "        snapshot_date,\n",
    "        LEAST(\n",
    "            DATE(snapshot_date),\n",
    "            IFNULL(DATE(acc_data.opendate),'9999-12-31'), \n",
    "            IFNULL(first_billpay_date, '9999-12-31'), \n",
    "            IFNULL(virtual_transaction_date, '9999-12-31'), \n",
    "            IFNULL(first_tsa_topup_date, '9999-12-31')\n",
    "        ) AS first_opened_date\n",
    "        FROM (SELECT DISTINCT cust_id,registration_date,snapshot_date from cust_onboarding_acc_data) acc \n",
    "        LEFT JOIN (SELECT cust_id as customer_id,ofdateopened opendate ,productid,account_type as accountdescription from cust_onboarding_acc_data \n",
    "        WHERE  account_type NOT IN ('Tonik Account','Tendo Individual Stash') AND productid in ('fixdep','savings','SaveForFuture') AND ofdateopened >= '2023-01-01' AND DATE(reccreatedon) < DATE(snapshot_date)\n",
    "       QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY reccreatedon asc) = 1) acc_data\n",
    "       ON acc.cust_id = acc_data.customer_id\n",
    "      LEFT JOIN utility_transaction_data\n",
    "        ON \n",
    "      acc.cust_id = utility_transaction_data.customer_id\n",
    "),\n",
    "first_product_data as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    --first_opened_date,\n",
    "    CASE\n",
    "        WHEN first_opened_date = DATE(opendate) THEN accountdescription\n",
    "        WHEN first_opened_date = first_billpay_date THEN 'Bills Pay'\n",
    "        WHEN first_opened_date = first_tsa_topup_date THEN 'TSA Top-Up'\n",
    "        WHEN first_opened_date = virtual_transaction_date THEN 'Virtual Transaction'\n",
    "        ELSE 'Unknown'\n",
    "    END AS first_product,\n",
    "    CASE\n",
    "        WHEN first_opened_date = DATE(opendate) and productid in ('fixdep','savings','SaveForFuture') THEN 'Deposit Users'\n",
    "        --WHEN first_opened_date = DATE(opendate) and productid NOT IN ('fixdep','savings','SaveForFuture') THEN 'Loan Users'\n",
    "        WHEN first_opened_date = first_tsa_topup_date or first_opened_date = virtual_transaction_date or first_opened_date = first_billpay_date THEN 'Utility Users'\n",
    "        ELSE 'Ghost Users'\n",
    "    END AS first_product_user_segment\n",
    "FROM\n",
    "    combined_data\n",
    "),\n",
    "loan_metrics as (\n",
    "SELECT \n",
    "input_customers.customerid,\n",
    "\n",
    "count(DISTINCT(IF (lmt.digitalLoanAccountId IS NOT NULL AND coalesce(lmt.termsAndConditionsSubmitDateTime,if (lmt.new_loan_type ='Flex-up',lmt.startApplyDateTime,lmt.termsAndConditionsSubmitDateTime)) is null,lmt.digitalLoanAccountId,NULL))) tx_incomplete_loan_apps,\n",
    "from input_customers\n",
    "JOIN `risk_credit_mis.loan_master_table` lmt on cast(lmt.customerId as string) = input_customers.customerid and COALESCE(DATE(termsAndConditionsSubmitDateTime),DATE(startApplyDateTime))  < DATE(input_customers.snapshot_date)\n",
    "group by 1\n",
    "),\n",
    "complete_deposit_metrics as (\n",
    "with  deposit_acc_main AS \n",
    "(\n",
    "   SELECT\n",
    "  a.OFDATEOPENED as ofdateopened,\n",
    "  IF(OFISCLOSED = 'Y',DATE_DIFF(date(ofdateclosed),date(OFDATEOPENED),day),NULL) as stash_duration,\n",
    "  registration_date,\n",
    "  duration,\n",
    "  reccreatedon,\n",
    "  a.cust_id as customer_id,\n",
    "  a.account_type,\n",
    "  a.accountid as ofstandardaccountid,\n",
    "  --balancedateasof,\n",
    "  b.clearedbalance,\n",
    "  a.OFISCLOSED as closed,\n",
    "  ff.status as td_status,\n",
    "  a.ofdateclosed,\n",
    "  autorollover,\n",
    "  snapshot_date,\n",
    "  FROM cust_onboarding_acc_data a\n",
    "  JOIN risk_mart.customer_balance b on a.accountid = b.accountid and date(balanceDateAsOf) = DATE_SUB(DATE(snapshot_date),INTERVAL 1 DAY)\n",
    "  --JOIN input_customers on input_customers.customerid = a.cust_id \n",
    "  LEFT JOIN `finastra_raw.fixturefeature` ff on ff.accountid = a.accountid\n",
    "  WHERE 1=1 \n",
    "  --and date(balancedateasof) = DATE_SUB(p.start_date,INTERVAL 1 DAY)\n",
    "  and productid in ('savings','fixdep','SaveForFuture') and  DATE(reccreatedon) < DATE(snapshot_date)\n",
    "  )\n",
    "  \n",
    ",deposit_days_diff_sub as (\n",
    "    SELECT customer_id,\n",
    "    'Between All Deposits' days_diff_type,\n",
    "    DATE_DIFF(LEAD(ofdateopened) OVER (PARTITION BY customer_id ORDER BY ofdateopened ASC),ofdateopened,DAY) days_bt_trans\n",
    "    FROM deposit_acc_main\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT customer_id,\n",
    "    'Between TDs' days_diff_type,\n",
    "    DATE_DIFF(LEAD(ofdateopened) OVER (PARTITION BY customer_id ORDER BY ofdateopened ASC),ofdateopened,DAY) days_bt_trans\n",
    "    FROM deposit_acc_main\n",
    "    WHERE account_type not like '%Stash%'\n",
    "\n",
    ")\n",
    ",dep_days_bt_trans_med AS \n",
    "(\n",
    "-- get the median days in between\n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "\n",
    "PERCENTILE_CONT(IF(days_diff_type='Between All Deposits',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) med_days_bw_new_dep_acct_open,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Between TDs',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) med_days_bw_td_acct_open,\n",
    "FROM deposit_days_diff_sub\n",
    ")\n",
    "\n",
    ", deposit_account_counts AS \n",
    "(\n",
    "#### Number of Stash and Time Deposit accounts that are still open until the observation date with balance >= 100\n",
    "SELECT\n",
    "customer_id,\n",
    "CASE WHEN SUM(autorollover) >= 1.0 THEN 1 ELSE 0 END AS tx_td_auto_roll_over_enabled,\n",
    "MAX((IF(account_type LIKE '%Time Deposit%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(snapshot_date)),duration,0))) AS td_max_duration,\n",
    "AVG((IF(account_type LIKE '%Time Deposit%'and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)) ,duration,0))) AS td_avg_duration,\n",
    "MIN((IF(account_type LIKE '%Time Deposit%'and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),duration,0))) AS td_min_duration,\n",
    "MAX((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),stash_duration,0))) AS stash_max_duration,\n",
    "AVG((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),stash_duration,0))) AS stash_avg_duration,\n",
    "MIN((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),stash_duration,0))) AS stash_min_duration,\n",
    "SUM(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '1' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),clearedbalance,NULL))) td_balance,\n",
    "SUM(DISTINCT(IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),clearedbalance,NULL))) stash_balance,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '4' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),ofstandardaccountid,NULL))) td_accounts_completed_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '9' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) > DATE(snapshot_date)),ofstandardaccountid,NULL))) td_accounts_broken_cnt,\n",
    "COUNT(DISTINCT(IF(account_type <> 'Tonik Account', ofstandardaccountid,NULL))) deposit_accs_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Stash%',ofstandardaccountid,NULL))) stash_accounts_opened_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%',ofstandardaccountid,NULL))) td_accounts_opened_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Stash%' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR  DATE(ofdateclosed) > DATE(snapshot_date)) ,ofstandardaccountid,NULL))) stash_accounts_closed_cnt,\n",
    "FROM \n",
    "deposit_acc_main\n",
    "where\n",
    "DATE(reccreatedon) < DATE(snapshot_date) and account_type <> 'Tonik Account'\n",
    "GROUP BY 1\n",
    ")\n",
    "\n",
    "SELECT DISTINCT \n",
    "a.customer_id,\n",
    "td_balance,\n",
    "stash_balance,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "deposit_accs_cnt,\n",
    "stash_accounts_opened_cnt,\n",
    "stash_accounts_closed_cnt,\n",
    "td_accounts_opened_cnt,\n",
    "td_accounts_completed_cnt,\n",
    "td_accounts_broken_cnt,\n",
    "med_days_bw_new_dep_acct_open,\n",
    "med_days_bw_td_acct_open,\n",
    "td_max_duration,\n",
    "td_min_duration,\n",
    "td_avg_duration,\n",
    "stash_max_duration,\n",
    "stash_avg_duration,\n",
    "stash_min_duration,\n",
    "FROM deposit_account_counts a\n",
    "--LEFT JOIN deposit_account_90th_day_counts d ON a.customer_id = d.customer_id\n",
    "LEFT JOIN dep_days_bt_trans_med b ON b.customer_id = a.customer_id\n",
    "ORDER BY 2 DESC\n",
    ")\n",
    "SELECT\n",
    "COALESCE(first_product_data.first_product,'Unknown') as tx_first_product,\n",
    "COALESCE(first_product_data.first_product_user_segment,'Ghost Users') tx_first_product_user_segment,\n",
    "acc.customerid as customer_id,\n",
    "onb_tsa_onboarding_datetime,\n",
    "onb_credo_inquiry_date,\n",
    "onb_credolabRefNumber,\n",
    "null as ln_disb_flag,\n",
    "null as ln_loan_type,\n",
    "null as ln_prod_type,\n",
    "null as ln_loan_appln_time,\n",
    "null as first_disb_loan_disbursed_time,\n",
    "null as first_applied_loan_appln_time,\n",
    "null as first_applied_loan_type,\n",
    "null as first_applied_loan_tenor,\n",
    "null as first_applied_loan_amount,\n",
    "null as first_applied_loan_decision,\n",
    "null as first_applied_product_type,\n",
    "null as last_applied_loan_appln_time,\n",
    "null as last_applied_loan_decision,\n",
    "null as last_applied_os_type,\n",
    "null as last_applied_loan_type,\n",
    "null as last_applied_loan_tenor,\n",
    "null as last_applied_loan_amount,\n",
    "null as last_applied_product_type,\n",
    "null as last_applied_crif_id,\n",
    "null as last_applied_cic_score,\n",
    "null as last_applied_credo_ref_no,\n",
    "null as last_applied_credo_score,\n",
    "null as last_applied_demo_score,\n",
    "null as last_applied_apps_score,\n",
    "null as ln_fpd30_flag,\n",
    "null as ln_mature_fpd30_flag,\n",
    "null as ln_mature_fspd30_flag,\n",
    "null as ln_fspd30_flag,\n",
    "null as first_reject_loan_appln_time,\n",
    "null as first_rejected_digitalloanaccountId,\n",
    "null as first_reject_loan_type,\n",
    "null as first_reject_loan_amount,\n",
    "null as first_reject_loan_tenor,\n",
    "null as last_reject_loan_appln_time,\n",
    "null as last_rejected_digitalloanaccountId,\n",
    "null as last_reject_loan_type,\n",
    "null as last_reject_loan_amount,\n",
    "null as last_reject_loan_tenor,\n",
    "dob_snapshot_date,\n",
    "days_since_credo_call_onb,\n",
    "days_since_credo_call_snapshot_date,\n",
    "onb_age,\n",
    "onb_doc_type,\n",
    "onb_osversion,\n",
    "c_credo_score,\n",
    "ln_self_dec_income,\n",
    "ln_marital_status,\n",
    "ln_education_level,\n",
    "onb_email,\n",
    "ln_industry_new,\n",
    "ln_employment_type_new,\n",
    "snapshot_date,\n",
    "ln_nature_of_work_new,\n",
    "ln_source_funds_new,\n",
    "onb_first_name,\n",
    "onb_middle_name,\n",
    "onb_last_name,\n",
    "ln_brand,\n",
    "ln_cnt_dependents,\n",
    "ln_doc_type,\n",
    "ln_osversion,\n",
    "ln_user_type,\n",
    "cust_status_flag,\n",
    "cust_status_close_date,\n",
    "0 as ln_ever_applied_flag,\n",
    "onb_city,\n",
    "onb_province,\n",
    "EXTRACT(YEAR FROM onb_tsa_onboarding_datetime) onb_year,\n",
    "EXTRACT(MONTH FROM onb_tsa_onboarding_datetime) onb_month_of_year,\n",
    "EXTRACT(WEEK FROM onb_tsa_onboarding_datetime) - EXTRACT(WEEK FROM DATE_TRUNC(onb_tsa_onboarding_datetime, MONTH)) + 1 onb_week_of_month,\n",
    "EXTRACT(DAY FROM onb_tsa_onboarding_datetime) onb_day_of_month,\n",
    "EXTRACT(TIME FROM onb_tsa_onboarding_datetime) onb_time_of_day,\n",
    "a.tx_cnt_cash_in_total,\n",
    "a.tx_cnt_cash_in_ob2t,\n",
    "a.tx_cnt_cash_in_ot2t,\n",
    "a.tx_amt_cash_in_total,\n",
    "a.tx_amt_cash_in_ob2t,\n",
    "a.tx_amt_cash_in_ot2t,\n",
    "a.tx_cnt_cash_out_total,\n",
    "a.tx_cnt_cash_out_billpay,\n",
    "a.tx_cnt_cash_out_cards,\n",
    "a.tx_cnt_cash_out_t2ot,\n",
    "a.tx_cnt_cash_out_t2ob,\n",
    "a.tx_amt_cash_out_total,\n",
    "a.tx_amt_cash_out_billpay,\n",
    "a.tx_amt_cash_out_cards,\n",
    "a.tx_amt_cash_out_t2ot,\n",
    "a.tx_amt_cash_out_t2ob,\n",
    "overall_avg_days_bt_trans tx_avg_days_bt_trans,\n",
    "net_cash_in_avg_days_bt_trans tx_avg_days_bt_cash_in_trans,\n",
    "net_cash_out_avg_days_bt_trans tx_avg_days_bt_cash_out_trans,\n",
    "overall_med_days_bt_trans tx_med_days_bt_trans,\n",
    "cash_in_med_days_bt_trans tx_med_days_bt_cash_in_trans,\n",
    "cash_out_med_days_bt_trans tx_med_days_bt_cash_out_trans,\n",
    "deposit_accs_cnt tx_deposit_accnt_cnt,\n",
    "stash_accounts_opened_cnt tx_stash_accnt_opened_cnt,\n",
    "stash_accounts_closed_cnt tx_stash_accnt_closed_cnt,\n",
    "stash_balance tx_stash_balance,\n",
    "td_accounts_opened_cnt tx_td_accnt_opened_cnt,\n",
    "td_accounts_completed_cnt tx_td_accnt_completed_cnt,\n",
    "td_accounts_broken_cnt tx_td_accnt_broken_cnt,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "td_balance tx_td_balance,\n",
    "td_max_duration,\n",
    "td_min_duration,\n",
    "td_avg_duration,\n",
    "stash_max_duration,\n",
    "stash_avg_duration,\n",
    "stash_min_duration,\n",
    "med_days_bw_new_dep_acct_open tx_med_days_bw_new_dep_acct_open,\n",
    "med_days_bw_td_acct_open tx_med_days_bw_td_acct_open,\n",
    "null as tx_cnt_completed_loans,\n",
    "null as tx_cnt_rejected_loans,\n",
    "null as tx_cnt_active_loans,\n",
    "null as tx_cnt_applied_loan,\n",
    "null as tx_cnt_approved_loans,\n",
    "null as tx_cnt_disbursed_loans,\n",
    "tx_incomplete_loan_apps as tx_cnt_incomplete_loan_apps,\n",
    "null as tx_cnt_installments_paid_tot,\n",
    "null as tx_amt_installments_paid_tot,\n",
    "null as tx_cnt_installments_paid_last_disb,\n",
    "null as tx_amt_installments_paid_last_disb,\n",
    "null as tx_cnt_fpd10_ever,\n",
    "null as tx_cnt_fpd30_ever,\n",
    "null as tx_cnt_fspd30_ever,\n",
    "null as tx_cnt_dpd_gt_5_ever,\n",
    "null as tx_max_ever_dpd,\n",
    "null as tx_max_dpd_30d,\n",
    "null as tx_max_dpd_60d,\n",
    "null as tx_max_dpd_90d,\n",
    "null as tx_max_dpd_120d,\n",
    "null as tx_max_dpd_150d,\n",
    "null as tx_max_dpd_180d,\n",
    "null as tx_max_current_dpd,\n",
    "null as ln_any_prev_disb_loan_sil_mobile_flag,\n",
    "CASE WHEN tdbk_referral_code_mtb.cust_id is not null THEN 1\n",
    "ELSE 0 END AS referral_flag\n",
    "\n",
    "FROM input_customers acc\n",
    "LEFT JOIN first_product_data ON first_product_data.customer_id = acc.customerid \n",
    "LEFT JOIN transactions_final a ON acc.customerid = a.customer_id\n",
    "LEFT JOIN days_bt_trans_avg b ON acc.customerid = b.customer_id \n",
    "LEFT JOIN days_bt_trans_med c ON acc.customerid = c.customer_id\n",
    "LEFT JOIN complete_deposit_metrics d on d.customer_id = acc.customerid\n",
    "LEFT JOIN loan_metrics ON cast(loan_metrics.customerid as string) = acc.customerid\n",
    "LEFT JOIN (SELECT cust_id,member_type,referral_type FROM dl_customers_db_raw.tdbk_referral_code_mtb\n",
    "WHERE tdbk_referral_code_mtb.member_type='REFEREE') tdbk_referral_code_mtb on  tdbk_referral_code_mtb.cust_id = acc.customerid\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd3ab0",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.trench2_never_applied_snapshot_call_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "create or replace table `worktable_data_analysis.trench2_never_applied_snapshot_call_count` as\n",
    "with input_customers as (\n",
    "SELECT \n",
    "customerId,\n",
    "onb_tsa_onboarding_datetime,\n",
    "onb_age,\n",
    "onb_email,\n",
    "ln_industry_new,\n",
    "ln_employment_type_new,\n",
    "DATE('2025-09-01') as snapshot_date,\n",
    "ln_nature_of_work_new,\n",
    "ln_doc_type,\n",
    "ln_osversion,\n",
    "onb_kyc_status,\n",
    "onb_latitude,\n",
    "onb_longitude,\n",
    "ln_user_type,\n",
    "ln_loan_type,\n",
    "ln_prod_type,\n",
    "ln_loan_applied_flag,\n",
    "FROM\n",
    "`risk_mart.tsa_onboarded_but_never_applied_loan_20230101_20250831`\n",
    "--where DATE_ADD(onb_tsa_onboarding_datetime,INTERVAL 90 DAY) <= '2025-04-15'\n",
    "),\n",
    "cust_emails as (\n",
    "  with temp_output as (\n",
    "select too.email as to_email, emailTranscript.from.email as from_email,\n",
    " \n",
    "CASE WHEN SPLIT(emailTranscript.from.email,'@')[SAFE_OFFSET(1)] like '%tonik%' THEN 'outbound'\n",
    "WHEN SPLIT(too.email,'@')[SAFE_OFFSET(1)] like '%tonik%' THEN 'inbound'\n",
    "ELSE 'outbound'\n",
    "END AS category,\n",
    "creationTime\n",
    "from `genesys_raw.emails`,\n",
    "unnest(emailTranscript) emailTranscript,unnest(emailTranscript.to) too\n",
    ")\n",
    "SELECT\n",
    "*,\n",
    "CASE WHEN category = 'outbound' THEN to_email\n",
    "WHEN category = 'inbound'THEN from_email\n",
    "WHEN from_email not like '%tonik%' THEN from_email\n",
    "END AS customer_email_address,\n",
    "FROM temp_output\n",
    "),\n",
    " \n",
    "cust_mobile_genesys_data as (\n",
    "SELECT\n",
    "input_customers.customerid,\n",
    "MAX(\n",
    "    CASE\n",
    "      WHEN DATE(call_history.callDatetime) between DATE_SUB(input_customers.snapshot_date, INTERVAL 90 DAY) and DATE(input_customers.snapshot_date) THEN 1\n",
    "      ELSE 0\n",
    "  END\n",
    "    ) AS flag_contactable_last90D,\n",
    "  count(\n",
    "    CASE\n",
    "      WHEN DATE(call_history.callDatetime) between DATE_SUB(input_customers.snapshot_date, INTERVAL 90 DAY) and DATE(input_customers.snapshot_date) THEN 1\n",
    "      ELSE null\n",
    "  END\n",
    "    ) AS count_contactable_last90D,\n",
    "COUNT(CASE WHEN calldirection = 'outbound' THEN 1 ELSE null END) AS outbound_call_count,\n",
    "COUNT(CASE WHEN calldirection = 'inbound' THEN 1 ELSE null END) AS inbound_call_count,\n",
    "\n",
    "FROM input_customers\n",
    "LEFT JOIN (SELECT DISTINCT cust_id,mobile_no, valid_from_dt FROM `datalake_worktables.customer_mobile_mail_dtls`\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY cust_id,mobile_no order by valid_from_dt asc) = 1\n",
    ") mobile_details on input_customers.customerid = mobile_details.cust_id and DATE(mobile_details.valid_from_dt) < DATE(snapshot_date)\n",
    "LEFT JOIN `risk_credit_mis.call_attempt_history_gensys` call_history ON RIGHT(mobile_details.mobile_no,10) = right(call_history.mobileNumber,10) and DATE(call_history.callDatetime) < DATE(input_customers.snapshot_date) and connected = 1 and mediaType = 'voice'\n",
    "GROUP BY 1\n",
    "),\n",
    "cust_email_genesys_data as (\n",
    "SELECT\n",
    "input_customers.customerid,\n",
    "COUNT(CASE WHEN category = 'outbound' THEN 1 ELSE null END) AS outbound_email_count,\n",
    "COUNT(CASE WHEN category = 'inbound'  THEN 1 ELSE null END) AS inbound_email_count\n",
    "FROM input_customers\n",
    "LEFT JOIN (SELECT DISTINCT cust_id,email, valid_from_dt FROM `datalake_worktables.customer_mobile_mail_dtls`\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY cust_id,email order by valid_from_dt asc) = 1\n",
    ") email_details on input_customers.customerid = email_details.cust_id and DATE(email_details.valid_from_dt) < DATE(snapshot_date)\n",
    "left JOIN cust_emails ON email_details.email = cust_emails.customer_email_address and DATE(cust_emails.creationTime) < DATE(input_customers.snapshot_date)\n",
    "GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "  input_customers.customerid,\n",
    "  input_customers.snapshot_date,\n",
    " flag_contactable_last90D,\n",
    " count_contactable_last90D,\n",
    "inbound_call_count,\n",
    "outbound_call_count,\n",
    "outbound_email_count,\n",
    "inbound_email_count,\n",
    "FROM input_customers\n",
    "LEFT JOIN cust_mobile_genesys_data on cust_mobile_genesys_data.customerid = CAST(input_customers.customerid AS STRING) --and cust_mobile_genesys_data.snapshot_date = input_customers.snapshot_date\n",
    "LEFT JOIN cust_email_genesys_data ON  cust_email_genesys_data.customerid = CAST(input_customers.customerid AS STRING) --and cust_email_genesys_data.snapshot_date = input_customers.snapshot_date\n",
    " \n",
    " ;\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab96e3",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.trench2_never_applied_snapshot_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "CREATE OR REPLACE TABLE worktable_data_analysis.trench2_never_applied_snapshot_events as\n",
    "with input_customers as ( \n",
    "SELECT \n",
    "customerId,\n",
    "onb_tsa_onboarding_datetime,\n",
    "onb_age,\n",
    "onb_email,\n",
    "ln_industry_new,\n",
    "ln_employment_type_new,\n",
    "DATE('2025-08-10') as snapshot_date,\n",
    "ln_nature_of_work_new,\n",
    "ln_doc_type,\n",
    "ln_osversion,\n",
    "onb_kyc_status,\n",
    "onb_latitude,\n",
    "onb_longitude,\n",
    "ln_user_type,\n",
    "ln_loan_type,\n",
    "ln_prod_type,\n",
    "ln_loan_applied_flag,\n",
    "FROM\n",
    "`risk_mart.tsa_onboarded_but_never_applied_loan_20230101_20250810`\n",
    "),\n",
    "af_link AS\n",
    "(\n",
    "  ## To get the AF ID and Customer ID Link (using the first install of a customer)\n",
    "  SELECT DISTINCT appsflyer_id, customer_user_id, install_time,snapshot_date\n",
    "  FROM `appsflyer_raw.in_app_events_report` in_apps_events\n",
    "  JOIN `dl_customers_db_raw.tdbk_customer_mtb` c ON c.cust_id = in_apps_events.customer_user_id\n",
    "  JOIN input_customers ON In_apps_events.customer_user_id = input_customers.customerid and DATE(in_apps_events._partitiondate) < DATE(input_customers.snapshot_date)\n",
    "  WHERE 1=1\n",
    "  AND customer_user_id IS NOT NULL\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_user_id ORDER BY install_time ASC) = 1\n",
    " \n",
    "  UNION ALL\n",
    " \n",
    "  SELECT DISTINCT appsflyer_id, customer_user_id, install_time,snapshot_date\n",
    "  FROM `appsflyer_raw.organic_in_app_events_report` organic_in_apps_events\n",
    "  JOIN `dl_customers_db_raw.tdbk_customer_mtb` c ON c.cust_id = organic_in_apps_events.customer_user_id\n",
    " JOIN input_customers ON organic_in_apps_events.customer_user_id = input_customers.customerid and DATE(organic_in_apps_events._partitiondate) < DATE(input_customers.snapshot_date)\n",
    "  WHERE 1=1\n",
    "  AND customer_user_id IS NOT NULL\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_user_id ORDER BY install_time ASC) = 1\n",
    ")\n",
    ", events AS\n",
    "(\n",
    "  SELECT DISTINCT\n",
    "  cust_id as customer_id,\n",
    "  --snapshot_date,\n",
    "COUNT(DISTINCT IF(event_name = 'App_Launch',event_uuid,NULL)) meng_no_of_logins,\n",
    "COUNT(DISTINCT IF(\n",
    "    event_name IN ('Loans_QL_Calculator'),moengagerefid,NULL)) meng_ql_calculator_count,\n",
    "COUNT(DISTINCT IF(\n",
    "    event_name IN ('Loans_QL_Calculator'),event_uuid,NULL)) meng_ql_calculator_tot_visit_cnt\n",
    "  FROM `dl_customers_db_raw.tdbk_customer_mtb` c\n",
    "  LEFT JOIN `moengage_raw.events_hourly` a ON c.cust_id = cast(a.customer_id as string)\n",
    " JOIN input_customers ON cast(a.customer_id as string) = input_customers.customerid \n",
    "  WHERE 1=1 and DATE(event_time) < DATE(input_customers.snapshot_date)\n",
    "  --AND event_name IN ('App_Launch','Loans_QL_Calculator','Loans_QL_Launch','Loans_SIL_Launch')\n",
    "  group by 1\n",
    "  ),\n",
    "\n",
    "campaign_data as\n",
    "(  SELECT DISTINCT a.customer_user_id, a.appsflyer_id, a.media_source, a.partner, a.campaign, a.Retargeting_Conversion_Type, \n",
    "  CASE\n",
    "  WHEN a.media_source = 'website_channel=website_ss_ui=true_ss_gtm_ui=true_ss_qr=c' THEN 'Website'\n",
    "  WHEN a.media_source IS NULL AND a.partner IS NULL AND a.campaign IS NULL THEN 'Organic'\n",
    "  ELSE b.source END source,\n",
    "CASE\n",
    "  WHEN a.media_source = 'website_channel=website_ss_ui=true_ss_gtm_ui=true_ss_qr=c' THEN 'Website'\n",
    "  WHEN a.media_source IS NULL AND a.partner IS NULL AND a.campaign IS NULL THEN 'Organic'\n",
    "  ELSE b.source_group END source_group,\n",
    "  FROM \n",
    "  (\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    'Install' Retargeting_Conversion_Type,\n",
    "    FROM `appsflyer_raw.in_app_events_report`\n",
    "    WHERE 1=1\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    'Install' Retargeting_Conversion_Type\n",
    "    FROM `appsflyer_raw.organic_in_app_events_report`\n",
    "    WHERE 1=1\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    Retargeting_Conversion_Type\n",
    "    FROM `appsflyer_raw.in_app_events_retarget`\n",
    "    WHERE 1=1\n",
    "  ) a\n",
    "  LEFT JOIN `prj-prod-dataplatform.worktable_datachampions.installs_attribution_mapping` b\n",
    "  ON \n",
    "    COALESCE(a.media_source,a.partner,a.campaign) = COALESCE(b.media_source,b.partner,b.campaign)\n",
    "\n",
    "  WHERE 1=1\n",
    " \n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY a.customer_user_id ORDER BY install_time DESC) = 1\n",
    "  ORDER BY customer_user_id\n",
    " ),\n",
    "final_output as (\n",
    "SELECT DISTINCT\n",
    "input_customers.customerId,\n",
    "--input_customers.snapshot_date,\n",
    "TIMESTAMP_DIFF(MIN(cust_mtb.created_dt),MIN(install_time),MINUTE) appsflyer_install_to_registration_minutes,\n",
    "FROM input_customers\n",
    "JOIN `dl_customers_db_raw.tdbk_customer_mtb` cust_mtb ON cust_mtb.cust_id = input_customers.customerId\n",
    "--LEFT JOIN events c ON c.customer_id = input_customers.customerId and input_customers.snapshot_date = c.snapshot_date\n",
    "LEFT JOIN af_link b ON b.customer_user_id = input_customers.customerId and input_customers.snapshot_date = b.snapshot_date\n",
    "--LEFT JOIN campaign_data ON campaign_data.customer_user_id = input_customers.customerId\n",
    "group by 1\n",
    ")\n",
    "SELECT \n",
    "final_output.*,\n",
    "meng_no_of_logins,\n",
    "meng_ql_calculator_count,\n",
    "COALESCE(meng_ql_calculator_tot_visit_cnt,0) meng_ql_calculator_tot_visit_cnt,\n",
    "CASE WHEN campaign_data.source_group = 'Organic' THEN 'Organic'\n",
    "ELSE 'InOrganic' END AS channel_source_group,\n",
    "source as marketing_source_name,\n",
    "from final_output\n",
    "LEFT JOIN campaign_data ON campaign_data.customer_user_id = final_output.customerId\n",
    "LEFT JOIN events c ON c.customer_id = final_output.customerId-- and final_output.ln_loan_appln_time = c.ln_loan_appln_time\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18614d",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.gamma_model_never_applied_snapshot_20250831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "create or replace table `worktable_data_analysis.gamma_model_never_applied_snapshot_20250831` as\n",
    "SELECT \n",
    "a.customer_id,\n",
    "null as digitalloanaccountid,\n",
    "dob_snapshot_date,\n",
    "days_since_credo_call_onb,\n",
    "days_since_credo_call_snapshot_date,\n",
    "onb_credolabRefNumber,\n",
    "onb_credo_inquiry_date,\n",
    "onb_email,\n",
    "onb_age,\n",
    "onb_city,\n",
    "onb_province,\n",
    "ln_industry_new,\n",
    "ln_employment_type_new,\n",
    "a.snapshot_date,\n",
    "ln_nature_of_work_new,\n",
    "ln_source_funds_new,\n",
    "onb_first_name,\n",
    "onb_middle_name,\n",
    "onb_last_name,\n",
    "ln_brand,\n",
    "ln_cnt_dependents,\n",
    "onb_doc_type,\n",
    "onb_osversion onb_osversion,\n",
    "c_credo_score,\n",
    "cust_status_flag,\n",
    "cust_status_close_date,\n",
    "ln_self_dec_income,\n",
    "ln_marital_status,\n",
    "ln_education_level,\n",
    "0 as ln_ever_applied_flag,\n",
    "tx_first_product,\n",
    "tx_first_product_user_segment,\n",
    "ln_user_type,\n",
    "ln_loan_type,\n",
    "ln_prod_type ln_product_type,\n",
    "CASE WHEN a.last_rejected_digitalloanaccountId is not null then 'Rejected_Before'\n",
    "else 'Never_Applied_Before' end as existing_user_type,\n",
    "onb_tsa_onboarding_datetime onboarding_date,\n",
    "a.ln_loan_appln_time,\n",
    "first_disb_loan_disbursed_time ln_loan_disb_time,\n",
    "ln_mature_fspd30_flag,\n",
    "--CASE WHEN LOANMATURITYDATE < secondDueDate THEN 0 ELSE ln_mature_fspd30_flag\n",
    "--END AS  ln_mature_fspd30_flag,\n",
    "ln_fspd30_flag,\n",
    "ln_mature_fpd30_flag,\n",
    "--CASE WHEN LOANMATURITYDATE < firstDueDate THEN 0 ELSE ln_mature_fpd30_flag\n",
    "--END AS ln_mature_fpd30_flag,\n",
    "ln_fpd30_flag,\n",
    "first_applied_loan_appln_time,\n",
    "first_applied_loan_decision,\n",
    "first_applied_loan_type,\n",
    "first_applied_product_type,\n",
    "first_applied_loan_amount,\n",
    "first_applied_loan_tenor,\n",
    "last_applied_loan_appln_time,\n",
    "last_applied_loan_decision,\n",
    "last_applied_loan_type,\n",
    "last_applied_product_type,\n",
    "last_applied_loan_amount,\n",
    "last_applied_loan_tenor,\n",
    "last_applied_os_type,\n",
    "last_applied_crif_id last_applied_digitalloanaccountId,\n",
    "last_applied_crif_id,\n",
    "last_applied_cic_score,\n",
    "last_applied_credo_ref_no,\n",
    "last_applied_credo_score,\n",
    "last_applied_demo_score,\n",
    "last_applied_apps_score,\n",
    "onb_year,\n",
    "onb_month_of_year,\n",
    "onb_week_of_month,\n",
    "onb_day_of_month,\n",
    "onb_time_of_day,\n",
    "tx_cnt_cash_in_total,\n",
    "tx_cnt_cash_in_ob2t,\n",
    "tx_cnt_cash_in_ot2t,\n",
    "tx_amt_cash_in_total,\n",
    "tx_amt_cash_in_ob2t,\n",
    "tx_amt_cash_in_ot2t,\n",
    "tx_cnt_cash_out_total,\n",
    "tx_cnt_cash_out_billpay,\n",
    "tx_cnt_cash_out_cards,\n",
    "tx_cnt_cash_out_t2ot,\n",
    "tx_cnt_cash_out_t2ob,\n",
    "tx_amt_cash_out_total,\n",
    "tx_amt_cash_out_billpay,\n",
    "tx_amt_cash_out_cards,\n",
    "tx_amt_cash_out_t2ot,\n",
    "tx_amt_cash_out_t2ob,\n",
    "tx_deposit_accnt_cnt,\n",
    "tx_stash_accnt_opened_cnt,\n",
    "tx_stash_accnt_closed_cnt,\n",
    "tx_stash_balance,\n",
    "tx_td_accnt_opened_cnt,\n",
    "tx_td_accnt_completed_cnt,\n",
    "tx_td_accnt_broken_cnt,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "tx_td_balance,\n",
    "td_max_duration tx_td_max_duration,\n",
    "td_min_duration tx_td_min_duration,\n",
    "td_avg_duration tx_td_avg_duration,\n",
    "stash_max_duration tx_stash_max_duration,\n",
    "stash_avg_duration tx_stash_avg_duration,\n",
    "stash_min_duration tx_stash_min_duration,\n",
    "tx_med_days_bw_new_dep_acct_open,\n",
    "tx_med_days_bw_td_acct_open,\n",
    "tx_avg_days_bt_trans,\n",
    "tx_avg_days_bt_cash_in_trans,\n",
    "tx_avg_days_bt_cash_out_trans,\n",
    "tx_med_days_bt_trans,\n",
    "tx_med_days_bt_cash_in_trans,\n",
    "tx_med_days_bt_cash_out_trans,\n",
    "tx_cnt_applied_loan,\n",
    "COALESCE(tx_cnt_rejected_loans,0) tx_cnt_rejected_loans,\n",
    "tx_cnt_approved_loans,\n",
    "tx_cnt_disbursed_loans,\n",
    "tx_cnt_completed_loans,\n",
    "tx_cnt_active_loans,\n",
    "tx_cnt_incomplete_loan_apps,\n",
    "referral_flag onb_referral_flag,\n",
    "flag_contactable_last90D cs_contactable_last_90d_flag,\n",
    "ln_any_prev_disb_loan_sil_mobile_flag,\n",
    "CASE WHEN appsflyer_install_to_registration_minutes < 0 THEN NULL\n",
    "ELSE appsflyer_install_to_registration_minutes END appsflyer_install_to_registration_minutes,\n",
    "meng_no_of_logins,\n",
    "meng_ql_calculator_count,\n",
    "meng_ql_calculator_tot_visit_cnt,\n",
    "channel_source_group,\n",
    "marketing_source_name,\n",
    "outbound_call_count cs_cnt_outbound_calls,\n",
    "inbound_call_count cs_cnt_inbound_calls,\n",
    "outbound_email_count cs_cnt_outbound_emails,\n",
    "inbound_email_count cs_cnt_inbound_emails,\n",
    "FROM \n",
    " `worktable_data_analysis.trench2_never_applied_snapshot_transactions` a\n",
    "LEFT JOIN `worktable_data_analysis.trench2_never_applied_snapshot_events` b on a.customer_id = b.customerid \n",
    "LEFT JOIN `worktable_data_analysis.trench2_never_applied_snapshot_call_count` contactability ON cast(contactability.customerid as string)= a.customer_id \n",
    "LEFT JOIN (select customer_id from `worktable_data_analysis.gamma_model_ever_applied_snapshot_20250831`) ever_applied on ever_applied.customer_id = a.customer_id\n",
    "where ever_applied.customer_id is null\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
