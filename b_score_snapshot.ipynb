{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9c3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858c7c8",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632a8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b5f2a",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fba9c1",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707a20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75766b0a",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3b76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751a30f",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac61d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eb04f",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080a4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8776f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1d410",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20c39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = 'risk_mart'\n",
    "\n",
    "\n",
    "al = f'applied_loans_20230101_{CURRENT_DATE}'\n",
    "altrans = f'applied_loans_20210701_{CURRENT_DATE}_trans'\n",
    "nal = f'tsa_onboarded_but_never_applied_loan_20230101_{CURRENT_DATE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b98dd",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.b_score_snapshot_customer_data_20250721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca47b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "CREATE OR REPLACE TABLE `worktable_data_analysis.b_score_snapshot_customer_data_20250721` as\n",
    "with eligible_customers as \n",
    "(\n",
    "with filtered_customers as (  \n",
    "SELECT  \n",
    "cast(customerid as string) customerid,\n",
    "coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) AS first_loan_disb_appln_date,\n",
    "FROM `risk_credit_mis.loan_master_table` lmt\n",
    "--JOIN `dl_customers_db_raw.tdbk_customer_mtb` cust on cust.cust_id = lmt.customerid \n",
    "WHERE flagDisbursement = 1 \n",
    "QUALIFY ROW_NUMBER() OVER(PARTITION BY customerId order by  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) ASC) = 1\n",
    ")\n",
    "SELECT customerid,\n",
    "first_loan_disb_appln_date,\n",
    "DATE('2025-07-21') ln_snapshot_date,\n",
    "datetime(created_dt,'Asia/Manila') as onb_tsa_onboarding_datetime,\n",
    "FROM filtered_customers\n",
    "JOIN (SELECT DISTINCT ubcustomercode from `finastra_raw.account` where accountdescription = 'Tonik Account' and closed = 'N') acc on filtered_customers.customerid = acc.ubcustomercode\n",
    "JOIN `dl_customers_db_raw.tdbk_customer_mtb` cust on cust.cust_id = filtered_customers.customerid\n",
    "WHERE first_loan_disb_appln_date <= '2025-06-21'\n",
    "),\n",
    "first_applied_loan_data as (\n",
    "  SELECT eligible_customers.customerId,\n",
    "  ln_snapshot_date,\n",
    "  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) AS first_applied_loan_appln_time,\n",
    "  new_loan_type AS first_applied_loan_type,\n",
    "  loanRequestTenure as first_applied_loan_tenor,\n",
    "  loanRequestAmount as first_applied_loan_amount,\n",
    "  case when applied_loans.loantype='BNPL' and store_type =1 then 'Appliance'\n",
    "    when applied_loans.loantype='BNPL' and store_type =2 then 'Mobile' \n",
    "    when applied_loans.loantype='BNPL' and store_type =3 then 'Mall' \n",
    "    when applied_loans.loantype='BNPL' and store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as first_applied_product_type,\n",
    "  from `risk_credit_mis.loan_master_table`  applied_loans\n",
    "JOIN eligible_customers\n",
    "ON CAST(applied_loans.customerid AS STRING) = eligible_customers.customerid and  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) < DATE(ln_snapshot_date)\n",
    "left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    "qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on applied_loans.purpleKey=sil_category.mer_refferal_code\n",
    "  QUALIFY ROW_NUMBER() OVER(PARTITION BY customerId,ln_snapshot_date order by  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) ASC) = 1\n",
    "),\n",
    "Reject_flag_data_new as (SELECT digitalLoanAccountId,applicationStatus,\n",
    "case when  applicationStatus IN ('EXPIRED', 'EXEMPT') and prev_applicationStatus='REJECT' THEN 1 when applicationStatus ='REJECT' THEN 1 else 0 end reject_flag,\n",
    "case when  applicationStatus IN ('ACCEPT', 'CANCELLED','EXEMPT','EXPIRED','REJECT') and prev_applicationStatus='APPROVED' THEN 1 when applicationStatus ='APPROVED' THEN 1 else 0 end approved_flag\n",
    "\n",
    " FROM (SELECT digitalLoanAccountId, applicationStatus, created_dt, LEAD(applicationStatus) OVER(PARTITION BY digitalLoanAccountId order by created_dt desc, statusTraceId desc) AS  prev_applicationStatus,LEAD(created_dt) OVER(PARTITION BY digitalLoanAccountId order by created_dt desc, statusTraceId desc) AS prev_created_dt,  row_number() over (partition by digitalLoanAccountId order by created_dt desc, statusTraceId desc ) rn  from dl_loans_db_raw.tdbk_status_trace ) where rn =1)\n",
    " ,\n",
    "last_applied_loan_data as (\n",
    "  SELECT input_customers.customerId,\n",
    "  ln_snapshot_date,\n",
    "  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) AS last_applied_loan_appln_time,\n",
    "  CASE WHEN lower(osversion_v2) like 'ios%' THEN 'iOS' ElSE 'Android' END  as last_applied_os_type,\n",
    "  new_loan_type AS last_applied_loan_type,\n",
    "  loanRequestTenure as last_applied_loan_tenor,\n",
    "  loanRequestAmount as last_applied_loan_amount,\n",
    "  lmt.digitalLoanAccountId as last_applied_crif_id,\n",
    "  case when lmt.loantype='BNPL' and store_type =1 then 'Appliance'\n",
    "    when lmt.loantype='BNPL' and store_type =2 then 'Mobile' \n",
    "    when lmt.loantype='BNPL' and store_type =3 then 'Mall' \n",
    "    when lmt.loantype='BNPL' and store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as last_applied_product_type,\n",
    "  CASE WHEN reject_flag = 1 or lmt.applicationStatus in ('REJECT','EXPIRED','RESET','EXEMPT','CANCELLED')  THEN 'REJECT'\n",
    "  WHEN approved_flag = 1 or lmt.applicationStatus in ('APPROVED','COMPLETED','ACTIVATED') THEN 'APPROVED'\n",
    "  ELSE 'REJECT' END AS last_applied_loan_decision,\n",
    "  cic_score as last_applied_cic_score,\n",
    "  COALESCE(eligible_customers.cic_called_flag,0) as last_applied_cic_called_flag,\n",
    "  COALESCE(eligible_customers.cic_hit_flag,0) as last_applied_cic_hit_flag,\n",
    "  beta_demo_score as last_applied_demo_score,\n",
    "  apps_score as last_applied_apps_score,\n",
    "  CASE WHEN  lmt.new_loan_type LIKE 'SIL%' THEN s_credo_score\n",
    "  WHEN lmt.new_loan_type in ('Quick','Flex','Flex-up','Big Loan','ACL TSA') THEN c_credo_score\n",
    "  END AS last_applied_credo_score,\n",
    "  lmt.credolabRefNumber as last_applied_credo_ref_no,\n",
    "eligible_customers.ln_user_type,\n",
    "eligible_customers.ln_mature_fpd30_flag,\n",
    "eligible_customers.ln_fpd30_flag,\n",
    "eligible_customers.ln_mature_fspd30_flag,\n",
    "eligible_customers.ln_fspd30_flag,\n",
    "eligible_customers.onb_osversion,\n",
    "cust_status_flag,\n",
    "cust_status_close_date,\n",
    "eligible_customers.credo_inquiry_date,\n",
    "eligible_customers.ln_self_dec_income,\n",
    "eligible_customers.ln_marital_status,\n",
    "eligible_customers.ln_education_level,\n",
    "eligible_customers.ln_nature_of_work_new,\n",
    "eligible_customers.onb_email_verified_flag,\n",
    "eligible_customers.onb_place_of_birth,\n",
    "eligible_customers.onb_doc_type,\n",
    "eligible_customers.onb_country,\n",
    "eligible_customers.onb_province,\n",
    "eligible_customers.onb_city,\n",
    "eligible_customers.onb_barangay,\n",
    "eligible_customers.onb_postalcode,\n",
    "eligible_customers.onb_latitude,\n",
    "eligible_customers.onb_longitude,\n",
    "eligible_customers.onb_kyc_status,\n",
    "eligible_customers.onb_kyc_status_upgrade_datetime,\n",
    "COALESCE(eligible_customers.ln_osversion,lmt.osversion_v2) ln_osversion,\n",
    "if(lower(eligible_customers.ln_osversion) like 'ios%','Apple', eligible_customers.ln_brand) ln_brand,\n",
    "eligible_customers.ln_cnt_dependents,\n",
    "eligible_customers.ln_source_funds_new,\n",
    "eligible_customers.ln_employment_type_new,\n",
    "eligible_customers.ln_industry_new,\n",
    "eligible_customers.ln_company_name,\n",
    "eligible_customers.ln_salary_scaled_income,\n",
    "eligible_customers.ln_vas_opted_flag,\n",
    "eligible_customers.ln_age,\n",
    "eligible_customers.ln_mobile_no,\n",
    "eligible_customers.ln_alt_mobile_no,\n",
    "eligible_customers.ln_province,\n",
    "eligible_customers.ln_city,\n",
    "eligible_customers.ln_barangay,\n",
    "eligible_customers.ln_latitude,\n",
    "eligible_customers.ln_longitude,\n",
    "eligible_customers.ln_doc_type,\n",
    "eligible_customers.ln_ref1_type,\n",
    "eligible_customers.onb_first_name,\n",
    "eligible_customers.onb_middle_name,\n",
    "eligible_customers.onb_last_name,\n",
    "eligible_customers.onb_age,\n",
    "eligible_customers.onb_gender,\n",
    "eligible_customers.onb_mobile_no,\n",
    "eligible_customers.onb_email,\n",
    "eligible_customers.ln_loan_applied_flag,\n",
    "eligible_customers.ln_facta_flag,\n",
    "eligible_customers.ln_dl_rule_reject_flag,\n",
    "eligible_customers.ln_taran_rule_reject_flag,\n",
    "eligible_customers.ln_taran_scorecard_reject_flag,\n",
    "eligible_customers.ln_cdd_reject_flag,\n",
    "eligible_customers.ln_marked_underwriter_check_flag,\n",
    "eligible_customers.ln_underwriting_reject_flag,\n",
    "eligible_customers.ln_approved_not_disb_flag,\n",
    "eligible_customers.ln_vas_used_flag,\n",
    "eligible_customers.ln_os_type,\n",
    "eligible_customers.ln_address,\n",
    "eligible_customers.ln_postal_code,\n",
    "eligible_customers.ln_doc_number,\n",
    "eligible_customers.ln_source_funds,\n",
    "eligible_customers.ln_employment_type,\n",
    "eligible_customers.ln_nature_of_work,\n",
    "eligible_customers.ln_industry,\n",
    "eligible_customers.ln_ref2_type,\n",
    "eligible_customers.onb_self_dec_income,\n",
    "eligible_customers.onb_company_name,\n",
    "  from `risk_credit_mis.loan_master_table` lmt\n",
    "JOIN eligible_customers input_customers ON cast(lmt.customerid as string) = input_customers.customerid\n",
    "LEFT JOIN risk_mart.applied_loans_20210701_20250721_trans eligible_customers on eligible_customers.digitalLoanAccountId = lmt.digitalLoanAccountId\n",
    "LEFT JOIN Reject_flag_data_new on Reject_flag_data_new.digitalLoanAccountId = lmt.digitalLoanAccountId\n",
    "LEFT JOIN `risk_mart.sil_risk_ds_master_20230101_20250223` ds on ds.digitalLoanAccountId = lmt.digitalLoanAccountId\n",
    "left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    "qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on lmt.purpleKey=sil_category.mer_refferal_code\n",
    "where coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) < DATE(input_customers.ln_snapshot_date)\n",
    "  QUALIFY ROW_NUMBER() OVER(PARTITION BY customerId,ln_snapshot_date order by  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) DESC) = 1\n",
    "),\n",
    "first_disb_loan_data as (\n",
    "  SELECT eligible_customers.customerId,\n",
    "  ln_snapshot_date,\n",
    "  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) AS first_disb_loan_appln_time,\n",
    "  disbursementDateTime as first_disb_loan_disb_time,\n",
    "  new_loan_type AS first_disb_loan_type,\n",
    "  loanRequestTenure as first_disb_loan_tenor,\n",
    "  loanRequestAmount as first_disb_loan_amount,\n",
    " case when applied_loans.loantype='BNPL' and store_type =1 then 'Appliance'\n",
    "    when applied_loans.loantype='BNPL' and store_type =2 then 'Mobile' \n",
    "    when applied_loans.loantype='BNPL' and store_type =3 then 'Mall' \n",
    "    when applied_loans.loantype='BNPL' and store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end first_disb_product_type\n",
    "   from `risk_credit_mis.loan_master_table`  applied_loans\n",
    "JOIN eligible_customers\n",
    "ON cast(applied_loans.customerid as string) = eligible_customers.customerid and  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) < DATE(ln_snapshot_date)\n",
    "left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    "qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on applied_loans.purpleKey=sil_category.mer_refferal_code\n",
    "WHERE applied_loans.flagDisbursement = 1\n",
    "  QUALIFY ROW_NUMBER() OVER(PARTITION BY customerId,ln_snapshot_date order by  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) ASC) = 1\n",
    "),\n",
    "last_disb_loan_data as (\n",
    "  SELECT eligible_customers.customerId,\n",
    "  ln_snapshot_date,\n",
    "  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime))  AS last_disb_loan_appln_time,\n",
    "  disbursementDateTime as last_disb_loan_disb_time,\n",
    "  new_loan_type AS last_disb_loan_type,\n",
    "  loanRequestTenure as last_disb_loan_tenor,\n",
    "  loanRequestAmount as last_disb_loan_amount,\n",
    "  applied_loans.loanaccountnumber as prev_loanAccountNumber,\n",
    "  applied_loans.digitalLoanAccountId as last_disb_crif_id,\n",
    " case when applied_loans.loantype='BNPL' and store_type =1 then 'Appliance'\n",
    "    when applied_loans.loantype='BNPL' and store_type =2 then 'Mobile' \n",
    "    when applied_loans.loantype='BNPL' and store_type =3 then 'Mall' \n",
    "    when applied_loans.loantype='BNPL' and store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as last_disb_product_type\n",
    "  from `risk_credit_mis.loan_master_table`  applied_loans\n",
    "JOIN eligible_customers\n",
    "ON cast(applied_loans.customerid as string) = eligible_customers.customerid and  coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) < ln_snapshot_date\n",
    "left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    "qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on applied_loans.purpleKey=sil_category.mer_refferal_code\n",
    "WHERE applied_loans.flagDisbursement = 1\n",
    "  QUALIFY ROW_NUMBER() OVER(PARTITION BY customerId,ln_snapshot_date order by coalesce(termsAndConditionsSubmitDateTime,if (new_loan_type ='Flex-up',startApplyDateTime,termsAndConditionsSubmitDateTime)) DESC) = 1\n",
    ")\n",
    "SELECT \n",
    " cast(eligible_customers.customerId as string) customerid,\n",
    " first_loan_disb_appln_date,\n",
    " --birth_date,\n",
    "    onb_tsa_onboarding_datetime,\n",
    "    --credo_inquiry_date,\n",
    "    DATE_DIFF(eligible_customers.ln_snapshot_date,DATE(onb_tsa_onboarding_datetime),DAY) dob_observation_date,\n",
    "    DATE_DIFF(DATE(onb_tsa_onboarding_datetime),credo_inquiry_date,DAY) days_since_credo_call_onb,\n",
    "    DATE_DIFF(eligible_customers.ln_snapshot_date,credo_inquiry_date,DAY) days_since_credo_call_loan_application,\n",
    "    eligible_customers.ln_snapshot_date,\n",
    "    first_applied_loan_data.* EXCEPT(customerid,ln_snapshot_date),\n",
    "    first_disb_loan_data.* EXCEPT(customerid,ln_snapshot_date),\n",
    "    last_applied_loan_data.* except(customerid,ln_snapshot_date),\n",
    "    last_disb_loan_data.* except(customerid,ln_snapshot_date),\n",
    "    --ln_mature_fpd30_flag,\n",
    "    --ln_fpd30_flag,\n",
    "    --repeat_loan_type,\n",
    "FROM eligible_customers\n",
    "LEFT JOIN first_applied_loan_data ON first_applied_loan_data.customerid = eligible_customers.customerid and first_applied_loan_data.ln_snapshot_date = eligible_customers.ln_snapshot_date\n",
    "LEFT JOIN first_disb_loan_data ON first_disb_loan_data.customerid = eligible_customers.customerid and first_disb_loan_data.ln_snapshot_date = eligible_customers.ln_snapshot_date\n",
    "LEFT JOIN last_applied_loan_data ON last_applied_loan_data.customerid = eligible_customers.customerid and last_applied_loan_data.ln_snapshot_date = eligible_customers.ln_snapshot_date\n",
    "LEFT JOIN last_disb_loan_data ON last_disb_loan_data.customerid = eligible_customers.customerid and last_disb_loan_data.ln_snapshot_date = eligible_customers.ln_snapshot_date\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd3ab0",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.b_score_snapshot_customer_transaction_data_20250721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "CREATE OR REPLACE TABLE `worktable_data_analysis.b_score_snapshot_customer_transaction_data_20250721` as\n",
    "with input_customers as(\n",
    "select * from `worktable_data_analysis.b_score_snapshot_customer_data_20250721`\n",
    "),\n",
    "cust_onboarding_acc_data as (\n",
    "    SELECT\n",
    "    DATE(opendate,'Asia/Manila') OFDATEOPENED,\n",
    "    DATE(closuredate,'Asia/Manila') ofdateclosed,\n",
    "    closed OFISCLOSED,\n",
    "    DATE(c.created_dt) registration_date,\n",
    "    c.created_dt as onboarding_date,\n",
    "    DATETIME(reccreatedon,'Asia/Manila') reccreatedon,\n",
    "    cust_id,\n",
    "    accountid,\n",
    "    productid,\n",
    "    accountdescription as account_type,\n",
    "    clearedbalance,\n",
    "    first_loan_disb_appln_date,\n",
    "    ln_snapshot_date\n",
    "    FROM `dl_customers_db_raw.tdbk_customer_mtb` c    \n",
    "    JOIN `finastra_raw.account` b ON c.cust_id = b.ubcustomercode\n",
    "    JOIN input_customers on c.cust_id = CAST(input_customers.customerid AS STRING)\n",
    "),\n",
    "main_transaction_data AS \n",
    "(\n",
    "     SELECT \n",
    "    transaction_date,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    b.cust_id customer_id,\n",
    "    a.accountid,\n",
    "    productid,\n",
    "    b.account_type,\n",
    "    transaction_code,\n",
    "    a.status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    input_customers.ln_snapshot_date,\n",
    "    transaction_datetime,\n",
    "    -- customer_transactions to get the transactions\n",
    "    FROM (SELECT * FROM cust_onboarding_acc_data WHERE account_type = 'Tonik Account') b\n",
    "    JOIN input_customers on b.cust_id = CAST(input_customers.customerid AS STRING)\n",
    "    LEFT JOIN `risk_mart.customer_transactions` a ON a.accountid = b.accountid\n",
    "    and a.transaction_date < input_customers.ln_snapshot_date and a.status = 'Success'\n",
    "    --and a.transaction_datetime < input_customers.ln_snapshot_date\n",
    "\n",
    "    WHERE 1=1\n",
    "   \n",
    "),\n",
    "\n",
    "\n",
    "#### Net Cash In ####\n",
    "  -- 1. Outside Tonik to TSA\n",
    "  -- 2. Other Tonik user to Own Tonik Account\n",
    "\n",
    "\n",
    "net_cash_in AS \n",
    "(\n",
    "  ## 1. Outside Tonik to TSA\n",
    "  SELECT\n",
    "    transaction_date,\n",
    "    transaction_datetime,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash In' main_transaction_type,\n",
    "    'Outside Tonik to TSA' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and credit and all coming from Tonik Account\n",
    "  AND credit_debit_indicator = 'CREDIT'\n",
    "  AND account_type = 'Tonik Account' and LOWER(core_narration) NOT LIKE '%blocking%' and transaction_code not like 'A0%'\n",
    "  AND transaction_code IN ('N01','IP2','XE2','00T','21C','P01')\n",
    "  -- 1. Outside Tonik to TSA conditions (all cash in)\n",
    "  AND inter_exter_flag = 'Outside Tonik'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 2. Other Tonik user to Own Tonik Account\n",
    "  SELECT\n",
    "    transaction_date,\n",
    "    transaction_datetime,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash In' main_transaction_type,\n",
    "    'Other Tonik Users to Town Tonik Account' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and credit and all coming from Tonik Account\n",
    "  AND credit_debit_indicator = 'CREDIT'\n",
    "  AND account_type = 'Tonik Account' and LOWER(core_narration) NOT LIKE '%blocking%' and transaction_code not like 'A0%'\n",
    "  AND transaction_code IN ('N01','IP2','XE2','00T','21C','P01')\n",
    "\n",
    "  -- 2. Other Tonik user to Own Tonik Account\n",
    "  AND inter_exter_flag = 'Inside Tonik'\n",
    "  AND core_narration LIKE '%Receive money from other Tonik Account%'\n",
    "  -- AND LEFT(core_narration,STRPOS(core_narration, \",\")-1) = 'Receive money from other Tonik Account'\n",
    ")\n",
    "\n",
    "#### Net Cash Out ####\n",
    "-- 1. Bills Pay\n",
    "-- 2. Card Transactions\n",
    "-- 3. Own TSA to other Tonik Users\n",
    "-- 4. TSA to Outside Tonik\n",
    "\n",
    ", net_cash_out AS \n",
    "(\n",
    "\n",
    " ## 1. Bills Pay\n",
    "  SELECT \n",
    "    transaction_date,\n",
    "    transaction_datetime,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Bills Pay' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND credit_debit_indicator = 'DEBIT'\n",
    "  AND LOWER(core_narration) NOT LIKE '%blocking%'\n",
    "\n",
    "  -- 1. Bills Pay\n",
    "  AND channel = 'Billspay'\n",
    "\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 2. Card Transactions (Cash Out)\n",
    "  SELECT\n",
    "    a.transaction_date,\n",
    "    transaction_datetime,\n",
    "    a.OFDATEOPENED,\n",
    "    a.OFISCLOSED,\n",
    "    a.registration_date,\n",
    "    a.transaction_id,\n",
    "    a.customer_id,\n",
    "    a.accountid,\n",
    "    a.account_type,\n",
    "    a.status,\n",
    "    a.channel,\n",
    "    a.credit_debit_indicator,\n",
    "    a.inter_exter_flag,\n",
    "    a.trx_amount,\n",
    "    a.core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Card Transactions (Cash Out)' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  -- 2. Card Transactions (Cash Out) -- using the table made above\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit and coming from tonik account\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND transaction_code like 'A0%' and core_narration not like '%Blocking%'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 3. Own TSA to other Tonik Users\n",
    "  SELECT DISTINCT\n",
    "    transaction_date,\n",
    "    transaction_datetime,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'Own TSA to Other Tonik Users' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND transaction_code not like 'A0%' and core_narration not like '%Blocking%'\n",
    "\n",
    "  -- 3. Own TSA to other Tonik Users\n",
    "  AND a.channel = 'Core transactions'\n",
    "  AND a.inter_exter_flag = 'Inside Tonik'\n",
    "  --AND LOWER(core_narration) LIKE '%send money to other tonik account%' \n",
    "  -- AND LOWER(core_narration) NOT LIKE '%scontri%'\n",
    "  -- AND LOWER(core_narration) NOT LIKE '%stash%'\n",
    "  -- AND LOWER(core_narration) NOT LIKE '%time deposit%'\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  ## 4. TSA to Outside Tonik (Other banks)\n",
    "  SELECT DISTINCT\n",
    "    transaction_date,\n",
    "    transaction_datetime,\n",
    "    OFDATEOPENED,\n",
    "    OFISCLOSED,\n",
    "    registration_date,\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    accountid,\n",
    "    account_type,\n",
    "    status,\n",
    "    channel,\n",
    "    credit_debit_indicator,\n",
    "    inter_exter_flag,\n",
    "    trx_amount,\n",
    "    core_narration,\n",
    "    'Net Cash Out' main_transaction_type,\n",
    "    'TSA to Outside Tonik (Other Banks)' sub_transaction_type,\n",
    "    ln_snapshot_date\n",
    "  FROM main_transaction_data a\n",
    "  WHERE 1=1\n",
    "  -- main conditions: should be a successful transaciton and debit\n",
    "  AND a.credit_debit_indicator = 'DEBIT'\n",
    "  AND a.account_type = 'Tonik Account'\n",
    "  AND core_narration not like '%Blocking%'\n",
    "\n",
    "  -- channels not in core transactions and billspay with the flag as outside tonik are sending to other banks\n",
    "  AND a.channel NOT IN  ('Core transactions','Billspay')\n",
    "  AND a.inter_exter_flag = 'Outside Tonik'\n",
    ")\n",
    "\n",
    ", transactions_sub AS \n",
    "(\n",
    "  -- merging the cash ins and cash outs\n",
    "  SELECT DISTINCT *\n",
    "  FROM net_cash_in \n",
    "  UNION ALL\n",
    "  SELECT DISTINCT *\n",
    "  FROM net_cash_out\n",
    ")\n",
    "\n",
    ", date_diff_sub AS \n",
    "(\n",
    "    -- to get the date difference between 2 transactions (cash in and cash out)\n",
    "    SELECT customer_id,\n",
    "    \n",
    "    'Overall' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id,ln_snapshot_date ORDER BY transaction_date,core_narration ASC),transaction_date,DAY) days_bt_trans,\n",
    "    ln_snapshot_date\n",
    "    \n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        ln_snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE transaction_date < ln_snapshot_date\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "    )\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- to get the date difference between 2 cash ins\n",
    "    SELECT customer_id,\n",
    "    'Cash In' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id,ln_snapshot_date ORDER BY transaction_date,core_narration ASC),transaction_date,DAY) days_bt_trans,\n",
    "    ln_snapshot_date\n",
    "    \n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        ln_snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE transaction_date < ln_snapshot_date\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "        AND main_transaction_type = 'Net Cash In'\n",
    "    )\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- to get the date difference between 2 cash outs\n",
    "    SELECT customer_id,\n",
    "    'Cash Out' days_diff_type,\n",
    "    DATE_DIFF(LEAD(transaction_date) OVER (PARTITION BY customer_id,ln_snapshot_date ORDER BY transaction_date,core_narration ASC),transaction_date,DAY) days_bt_trans,\n",
    "    ln_snapshot_date\n",
    "    FROM \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        main_transaction_type,\n",
    "        ln_snapshot_date,\n",
    "        core_narration\n",
    "        FROM transactions_sub\n",
    "        WHERE transaction_date < ln_snapshot_date\n",
    "        --   AND customer_id IN ('2077378','2081999','2475220','2485072')\n",
    "        AND main_transaction_type = 'Net Cash Out'\n",
    "    )\n",
    ")\n",
    "\n",
    ", days_bt_trans_avg AS \n",
    "(\n",
    "-- get the average days in between \n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "ln_snapshot_date,\n",
    "AVG(IF(days_diff_type='Overall',days_bt_trans,NULL)) overall_avg_days_bt_trans,\n",
    "AVG(IF(days_diff_type='Cash In',days_bt_trans,NULL)) net_cash_in_avg_days_bt_trans,\n",
    "AVG(IF(days_diff_type='Cash Out',days_bt_trans,NULL)) net_cash_out_avg_days_bt_trans\n",
    "FROM date_diff_sub\n",
    "GROUP BY 1,2\n",
    ")\n",
    "\n",
    ", days_bt_trans_med AS \n",
    "(\n",
    "-- get the median days in between\n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "ln_snapshot_date,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Overall',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) overall_med_days_bt_trans,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Cash In',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) cash_in_med_days_bt_trans,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Cash Out',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) cash_out_med_days_bt_trans,\n",
    "FROM date_diff_sub\n",
    ")\n",
    "\n",
    ", transactions_final AS \n",
    "(\n",
    "SELECT DISTINCT\n",
    "acc.customerid as customer_id,\n",
    "acc.ln_snapshot_date,\n",
    "## Number of transactions within the observation window (x days from onboarding date),\n",
    "## Cash In Count Details\n",
    "COUNT(DISTINCT(IF(main_transaction_type = 'Net Cash In',transaction_id,NULL))) tx_cnt_cash_in_total,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type='Outside Tonik to TSA' ,transaction_id,NULL))) tx_cnt_cash_in_ob2t,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type='Other Tonik Users to Town Tonik Account' ,transaction_id,NULL))) tx_cnt_cash_in_ot2t,\n",
    "\n",
    "## Cash In Amount Details\n",
    "SUM(IF(main_transaction_type = 'Net Cash In',trx_amount,0)) tx_amt_cash_in_total,\n",
    "SUM((IF(sub_transaction_type='Outside Tonik to TSA' ,trx_amount,0))) tx_amt_cash_in_ob2t,\n",
    "SUM((IF(sub_transaction_type='Other Tonik Users to Town Tonik Account',trx_amount,0))) tx_amt_cash_in_ot2t,\n",
    "\n",
    "## Cash Out Count Details\n",
    "COUNT(DISTINCT(IF(main_transaction_type = 'Net Cash Out',transaction_id,NULL))) tx_cnt_cash_out_total,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Bills Pay' ,transaction_id,NULL))) tx_cnt_cash_out_billpay,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Card Transactions (Cash Out)' ,transaction_id,NULL))) tx_cnt_cash_out_cards,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'Own TSA to Other Tonik Users' ,transaction_id,NULL))) tx_cnt_cash_out_t2ot,\n",
    "COUNT(DISTINCT(IF(sub_transaction_type= 'TSA to Outside Tonik (Other Banks)' ,transaction_id,NULL))) tx_cnt_cash_out_t2ob,\n",
    "\n",
    "## Cash Out Amount Details\n",
    "SUM(IF(main_transaction_type = 'Net Cash Out',trx_amount,0)) tx_amt_cash_out_total,\n",
    "SUM(IF(sub_transaction_type= 'Bills Pay' ,trx_amount,0)) tx_amt_cash_out_billpay,\n",
    "SUM(IF(sub_transaction_type= 'Card Transactions (Cash Out)' ,trx_amount,0)) tx_amt_cash_out_cards,\n",
    "SUM(IF(sub_transaction_type= 'Own TSA to Other Tonik Users' ,trx_amount,0)) tx_amt_cash_out_t2ot,\n",
    "SUM(IF(sub_transaction_type= 'TSA to Outside Tonik (Other Banks)' ,trx_amount,0)) tx_amt_cash_out_t2ob,\n",
    "FROM input_customers acc \n",
    "LEFT JOIN transactions_sub a ON CAST(acc.customerid AS STRING) = a.customer_id  and acc.ln_snapshot_date = a.ln_snapshot_date\n",
    "\n",
    "GROUP BY 1,2\n",
    "ORDER BY 2 \n",
    "\n",
    "),\n",
    "Reject_flag_data_new as (SELECT digitalLoanAccountId, \n",
    "case when  applicationStatus IN ('EXPIRED', 'EXEMPT') and prev_applicationStatus='REJECT' THEN 1 when applicationStatus ='REJECT' THEN 1 else 0 end reject_flag,\n",
    "case when  applicationStatus IN ('ACCEPT', 'CANCELLED','EXEMPT','EXPIRED','REJECT') and prev_applicationStatus='APPROVED' THEN 1 when applicationStatus ='APPROVED' THEN 1 else 0 end approved_flag\n",
    "\n",
    " FROM (SELECT digitalLoanAccountId, applicationStatus, created_dt, LEAD(applicationStatus) OVER(PARTITION BY digitalLoanAccountId order by created_dt desc, statusTraceId desc) AS  prev_applicationStatus,LEAD(created_dt) OVER(PARTITION BY digitalLoanAccountId order by created_dt desc, statusTraceId desc) AS prev_created_dt,  row_number() over (partition by digitalLoanAccountId order by created_dt desc, statusTraceId desc ) rn  from dl_loans_db_raw.tdbk_status_trace ) where rn =1)\n",
    " ,\n",
    "delinquency_data as (\n",
    "select loanAccountNumber,\n",
    "case when obs_min_inst_def10 >=1 and min_inst_def10 =1 then 1 else 0 end deffpd10,\n",
    "case when obs_min_inst_def30 >=1 and min_inst_def30 =1 then 1 else 0 end deffpd30,\n",
    "case when obs_min_inst_def30 >=2 and min_inst_def30  in (1,2) then 1 else 0 end deffspd30,\n",
    "case when obs_min_inst_def30 >=3 and min_inst_def30 in (1,2,3) then 1 else 0 end deffstpd30,\n",
    "case when obs_min_inst_def10 >=1 then 1 else 0 end flg_mature_fpd10,\n",
    "case when obs_min_inst_def30 >=1 then 1 else 0 end flg_mature_fpd30,\n",
    "case when obs_min_inst_def30 >=2 then 1 else 0 end flg_mature_fspd_30,\n",
    "case when obs_min_inst_def30 >=3 then 1 else 0 end flg_mature_fstpd_30\n",
    "from risk_credit_mis.loan_deliquency_data\n",
    "),\n",
    "\n",
    "fr_dpd as (\n",
    "      select\n",
    "      input_customers.customerid,\n",
    "      ln_snapshot_date,\n",
    "      MAX(fr.Max_ever_DPD) as max_ever_dpd,\n",
    "      /*MAX(CASE WHEN fr.loanAccountNumber = input_customers.loanAccountNumber THEN fr.Max_ever_DPD ELSE NULL END) AS max_current_dpd,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 30 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_30d,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 60 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_60d,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 90 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_90d,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 120 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_120d,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 150 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_150d,\n",
    "      MAX(CASE WHEN date(sourceDataAsOf) BETWEEN DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 180 DAY) AND DATE(input_customers.ln_snapshot_date) THEN fr.Max_ever_DPD ELSE NULL END) max_ever_dpd_180d,*/\n",
    "      from input_customers\n",
    "JOIN `risk_credit_mis.loan_master_table` lmt on cast(lmt.customerId as string) = input_customers.customerid and COALESCE(DATE(termsAndConditionsSubmitDateTime),DATE(startApplyDateTime))  < DATE(input_customers.ln_snapshot_date)\n",
    "      left join `prj-prod-dataplatform.risk_credit_mis.loan_bucket_flow_report_core`  fr on  fr.loanAccountNumber = lmt.loanAccountNumber and date(sourceDataAsOf) < date(ln_snapshot_date)\n",
    "      where lmt.flagDisbursement = 1\n",
    "      group by 1,2\n",
    " ),\n",
    "loan_metrics as (\n",
    "SELECT \n",
    "input_customers.customerid,\n",
    "--input_customers.loanAccountNumber,\n",
    "input_customers.ln_snapshot_date,\n",
    "fr_dpd.max_ever_dpd,\n",
    "/*max_current_dpd,\n",
    "max_ever_dpd_30d,\n",
    "max_ever_dpd_60d,\n",
    "max_ever_dpd_90d,\n",
    "max_ever_dpd_120d,\n",
    "max_ever_dpd_150d,\n",
    "max_ever_dpd_180d,*/\n",
    "--COUNT(CASE WHEN installmentPaidAmount > 0 AND prev_loanAccountNumber = a.loanAccountNumber and lastPaymentDate < DATE(input_customers.ln_snapshot_date) THEN 1 ELSE NULL END) AS cnt_installments_paid_last_disb,\n",
    "--SUM(CASE WHEN installmentPaidAmount > 0 AND prev_loanAccountNumber = a.loanAccountNumber and lastPaymentDate < DATE(input_customers.ln_snapshot_date) THEN installmentPaidAmount ELSE 0 END) AS total_amt_installments_paid_last_disb,\n",
    "COUNT(CASE WHEN installmentPaidAmount >0 AND DPDwoToleranceCustom > 0 and (lastPaymentDate < DATE(input_customers.ln_snapshot_date) OR isDelinquent =1) THEN 1 ELSE NULL END) AS tx_cnt_installments_paid_tot_with_dpd,\n",
    "SUM(CASE WHEN installmentPaidAmount >0 AND DPDwoToleranceCustom > 0 and (lastPaymentDate < DATE(input_customers.ln_snapshot_date) OR isDelinquent =1) THEN installmentPaidAmount ELSE 0 END) AS tx_amt_installments_paid_tot_with_dpd,\n",
    "--COUNT(CASE WHEN installmentPaidAmount >0 AND prev_loanAccountNumber = a.loanAccountNumber AND DPDwoToleranceCustom > 0 and (lastPaymentDate < DATE(input_customers.ln_snapshot_date) OR isDelinquent =1) THEN 1 ELSE NULL END) AS tx_cnt_installments_paid_last_disb_withdpd,\n",
    "SUM(installmentAmount) as tx_total_due_amount,\n",
    "CASE WHEN COUNT(CASE WHEN flagDisbursement = 1 AND new_loan_type LIKE 'SIL%' THEN 1 ELSE NULL END) > 1 THEN 1 ELSE 0 END AS ln_any_prev_disb_loan_sil_mobile_flag,\n",
    "COUNT(CASE WHEN installmentPaidAmount > 0  THEN 1 ELSE NULL END) AS cnt_installments_paid,\n",
    "\n",
    "SUM(CASE WHEN installmentPaidAmount > 0  THEN installmentPaidAmount ELSE 0 END) AS total_amt_installments_paid,\n",
    "count(DISTINCT(IF(coalesce(lmt.termsAndConditionsSubmitDateTime,if (lmt.new_loan_type ='Flex-up',lmt.startApplyDateTime,lmt.termsAndConditionsSubmitDateTime)) is not null,lmt.digitalLoanAccountId,NULL))) tx_cnt_applied_loan_apps,\n",
    "count(DISTINCT(IF(Reject_flag_data_new.reject_flag = 1,lmt.digitalLoanAccountId,NULL))) tx_cnt_rejected_loan_apps,\n",
    "count(DISTINCT(IF( applicationStatus in ('COMPLETED','ACTIVATED','APPROVED'),lmt.digitalLoanAccountId,NULL))) tx_cnt_approved_loan_apps,\n",
    "count(DISTINCT(IF(la.LOANSTATUS IN('Completed','Settled'),lmt.digitalLoanAccountId,NULL))) tx_cnt_completed_loan_apps,\n",
    "count(DISTINCT(IF(disbursementDateTime is not null ,lmt.digitalLoanAccountId,NULL))) tx_cnt_disbursed_loan_apps,\n",
    "count(DISTINCT(IF( la.LOANSTATUS IN('Normal','In Arrears'),lmt.digitalLoanAccountId,NULL))) tx_cnt_active_loan_apps,\n",
    "count(DISTINCT(IF (lmt.digitalLoanAccountId IS NOT NULL AND coalesce(lmt.termsAndConditionsSubmitDateTime,if (lmt.new_loan_type ='Flex-up',lmt.startApplyDateTime,lmt.termsAndConditionsSubmitDateTime)) is null,lmt.digitalLoanAccountId,NULL))) tx_incomplete_loan_apps,\n",
    "MIN(CASE WHEN la.LOANSTATUS IN('Completed','Settled') THEN DATE_DIFF(LOANMATURITYDATE,DATEOFDISBURSEMENT, DAY) ELSE NULL END) AS tx_min_age_completed_loans,\n",
    "MAX(CASE WHEN la.LOANSTATUS IN('Completed','Settled') THEN DATE_DIFF(LOANMATURITYDATE,DATEOFDISBURSEMENT, DAY) ELSE NULL END) AS tx_max_age_completed_loans,\n",
    "AVG(CASE WHEN la.LOANSTATUS IN('Completed','Settled') THEN DATE_DIFF(LOANMATURITYDATE,DATEOFDISBURSEMENT, DAY) ELSE NULL END) AS tx_avg_age_completed_loans,\n",
    "count(CASE WHEN defFPD05 = 1 THEN 1 ELSE NULL END) as cnt_fpd5,\n",
    "count(CASE WHEN delinquency_data.defFPD10 = 1 THEN 1 ELSE NULL END) as cnt_fpd10,\n",
    "count(CASE WHEN delinquency_data.defFPD30 = 1 THEN 1 ELSE NULL END) as cnt_fpd30,\n",
    "count(CASE WHEN delinquency_data.deffspd30 = 1 THEN 1 ELSE NULL END) as cnt_fspd30,\n",
    "COUNT(case when DPDwoToleranceCustom_DPD > 1 THEN 1 ELSE NULL END) AS cnt_dpd_gt_1,\n",
    "COUNT(case when DPDwoToleranceCustom_DPD > 5 THEN 1 ELSE NULL END) AS cnt_dpd_gt_5,\n",
    "from input_customers\n",
    "JOIN `risk_credit_mis.loan_master_table` lmt on cast(lmt.customerId as string) = input_customers.customerid and COALESCE(DATE(termsAndConditionsSubmitDateTime),DATE(startApplyDateTime))  < DATE(input_customers.ln_snapshot_date)\n",
    "LEFT JOIN `risk_credit_mis.loan_installments_table` a on a.loanAccountNumber = lmt.loanAccountNumber and COALESCE(installmentDueDate) < DATE(input_customers.ln_snapshot_date)\n",
    "LEFT JOIN Reject_flag_data_new ON Reject_flag_data_new.digitalLoanAccountId = lmt.digitalLoanAccountId\n",
    "LEFT JOIN delinquency_data on delinquency_data.loanAccountNumber = lmt.loanAccountNumber\n",
    "LEFT JOIN core_raw.loan_accounts la ON la.AccountNumber = lmt.loanAccountNumber and _PARTITIONDATE = DATE_SUB(DATE(input_customers.ln_snapshot_date),INTERVAL 1 DAY)\n",
    "LEFT JOIN fr_dpd on  fr_dpd.customerid  = input_customers.customerid and fr_dpd.ln_snapshot_date = input_customers.ln_snapshot_date\n",
    "group by all\n",
    "),\n",
    "\n",
    "utility_transaction_data AS (\n",
    "        SELECT \n",
    "        customer_id,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code IN ('BP1', 'BP2', 'BP3', 'BP4') THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS first_billpay_date,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code like 'A0%' AND core_narration NOT LIKE '%Blocking%' THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS virtual_transaction_date,\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN transaction_code IN ('21C', 'N01', 'IP2', 'XE2','P01') THEN 1 ELSE 0 END) > 0 \n",
    "                THEN MIN(transaction_date) \n",
    "            ELSE NULL \n",
    "        END AS first_tsa_topup_date\n",
    "    FROM \n",
    "        main_transaction_data\n",
    "    GROUP BY 1\n",
    "),\n",
    "\n",
    "combined_data AS (\n",
    "    SELECT \n",
    "        COALESCE(acc.cust_id,acc_data.customer_id,utility_transaction_data.customer_id) customer_id,\n",
    "        productid,\n",
    "        accountdescription,\n",
    "        acc_data.opendate as opendate ,\n",
    "        first_billpay_date,\n",
    "        virtual_transaction_date,\n",
    "        first_tsa_topup_date,\n",
    "        first_loan_disb_appln_date,\n",
    "        ln_snapshot_date,\n",
    "        LEAST(\n",
    "            DATE(ln_snapshot_date),\n",
    "            IFNULL(DATE(acc_data.opendate),'9999-12-31'), \n",
    "            IFNULL(first_billpay_date, '9999-12-31'), \n",
    "            IFNULL(virtual_transaction_date, '9999-12-31'), \n",
    "            IFNULL(first_tsa_topup_date, '9999-12-31')\n",
    "        ) AS first_opened_date\n",
    "        FROM (SELECT DISTINCT cust_id,registration_date,first_loan_disb_appln_date,ln_snapshot_date from cust_onboarding_acc_data) acc \n",
    "        LEFT JOIN (SELECT cust_id as customer_id,ofdateopened opendate ,productid,account_type as accountdescription from cust_onboarding_acc_data \n",
    "        WHERE  account_type NOT IN ('Tonik Account','Tendo Individual Stash') AND ofdateopened >= '2023-01-01' AND reccreatedon < ln_snapshot_date\n",
    "       QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY reccreatedon asc) = 1) acc_data\n",
    "       ON acc.cust_id = acc_data.customer_id\n",
    "      LEFT JOIN utility_transaction_data\n",
    "        ON \n",
    "      acc.cust_id = utility_transaction_data.customer_id\n",
    "),\n",
    "first_product_data as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    ln_snapshot_date,\n",
    "    --first_opened_date,\n",
    "    CASE\n",
    "        WHEN first_opened_date = DATE(opendate) THEN accountdescription\n",
    "        WHEN first_opened_date = first_billpay_date THEN 'Bills Pay'\n",
    "        WHEN first_opened_date = first_tsa_topup_date THEN 'TSA Top-Up'\n",
    "        WHEN first_opened_date = virtual_transaction_date THEN 'Virtual Transaction'\n",
    "        ELSE 'Unknown'\n",
    "    END AS first_product,\n",
    "    CASE\n",
    "        WHEN first_opened_date = DATE(opendate) and productid NOT IN ('fixdep','savings','SaveForFuture') THEN 'Loan Users'\n",
    "        WHEN first_opened_date = DATE(opendate) and productid in ('fixdep','savings','SaveForFuture') THEN 'Deposit Users'\n",
    "        --WHEN first_opened_date = DATE(opendate) and productid NOT IN ('fixdep','savings','SaveForFuture') THEN 'Loan Users'\n",
    "        WHEN first_opened_date = first_tsa_topup_date or first_opened_date = virtual_transaction_date or first_opened_date = first_billpay_date THEN 'Utility Users'\n",
    "        ELSE 'Ghost Users'\n",
    "    END AS first_product_user_segment\n",
    "FROM\n",
    "    combined_data\n",
    "),\n",
    "\n",
    "complete_deposit_metrics as (\n",
    "with  deposit_acc_main AS \n",
    "(\n",
    "  SELECT\n",
    "  a.OFDATEOPENED as ofdateopened,\n",
    "  IF(OFISCLOSED = 'Y',DATE_DIFF(date(ofdateclosed),date(OFDATEOPENED),day),NULL) as stash_duration,\n",
    "  registration_date,\n",
    "  duration,\n",
    "  a.cust_id as customer_id,\n",
    "  a.account_type,\n",
    "  reccreatedon,\n",
    "  a.accountid as ofstandardaccountid,\n",
    "  --balancedateasof,\n",
    "  b.clearedbalance,\n",
    "  a.OFISCLOSED as closed,\n",
    "  ff.status as td_status,\n",
    "  a.ofdateclosed,\n",
    "  autorollover,\n",
    "  ln_snapshot_date,\n",
    " FROM cust_onboarding_acc_data a\n",
    "  LEFT JOIN risk_mart.customer_balance b on a.accountid = b.accountid and date(balanceDateAsOf) = DATE_SUB(DATE(ln_snapshot_date),INTERVAL 1 DAY)\n",
    "  --JOIN input_customers on input_customers.customerid = a.cust_id \n",
    "  LEFT JOIN `finastra_raw.fixturefeature` ff on ff.accountid = a.accountid\n",
    "  WHERE 1=1 \n",
    "  --and date(balancedateasof) = DATE_SUB(p.start_date,INTERVAL 1 DAY)\n",
    "  and productid in ('savings','fixdep','SaveForFuture') and  reccreatedon < ln_snapshot_date\n",
    "  )\n",
    "  \n",
    ",deposit_days_diff_sub as (\n",
    "    SELECT customer_id,\n",
    "    ln_snapshot_date,\n",
    "    'Between All Deposits' days_diff_type,\n",
    "    DATE_DIFF(LEAD(ofdateopened) OVER (PARTITION BY customer_id ORDER BY ofdateopened ASC),ofdateopened,DAY) days_bt_trans\n",
    "    FROM deposit_acc_main\n",
    "    WHERE account_type <> 'Tonik Account'\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT customer_id,\n",
    "    ln_snapshot_date,\n",
    "    'Between TSA AND TD' days_diff_type,\n",
    "    DATE_DIFF(LEAD(ofdateopened) OVER (PARTITION BY customer_id ORDER BY ofdateopened ASC),ofdateopened,DAY) days_bt_trans\n",
    "    FROM deposit_acc_main\n",
    "    WHERE account_type not like '%Stash%'\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT customer_id,\n",
    "    ln_snapshot_date,\n",
    "    'Between TDs' days_diff_type,\n",
    "    DATE_DIFF(LEAD(ofdateopened) OVER (PARTITION BY customer_id ORDER BY ofdateopened ASC),ofdateopened,DAY) days_bt_trans\n",
    "    FROM deposit_acc_main\n",
    "    WHERE account_type like '%Time Deposit%'\n",
    "\n",
    ")\n",
    ",dep_days_bt_trans_med AS \n",
    "(\n",
    "-- get the median days in between\n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "ln_snapshot_date,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Between All Deposits',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) med_days_bw_new_dep_acct_open,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Between TDs',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) med_days_bw_td_acct_open,\n",
    "PERCENTILE_CONT(IF(days_diff_type='Between TSA AND TD',days_bt_trans,NULL), .50) OVER (PARTITION BY customer_id) med_days_bw_td_tsa_acct_open,\n",
    "FROM deposit_days_diff_sub\n",
    ")\n",
    "\n",
    ", deposit_account_counts AS \n",
    "(\n",
    "#### Number of Stash and Time Deposit accounts that are still open until the observation date with balance >= 100\n",
    "SELECT DISTINCT\n",
    "customer_id,\n",
    "ln_snapshot_date,\n",
    "CASE WHEN SUM(autorollover) >= 1.0 THEN 1 ELSE 0 END AS tx_td_auto_roll_over_enabled,\n",
    "MAX((IF(account_type LIKE '%Time Deposit%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),duration,0))) AS td_max_duration,\n",
    "AVG((IF(account_type LIKE '%Time Deposit%'and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)) ,duration,0))) AS td_avg_duration,\n",
    "MIN((IF(account_type LIKE '%Time Deposit%'and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),duration,0))) AS td_min_duration,\n",
    "MAX((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),stash_duration,0))) AS stash_max_duration,\n",
    "AVG((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),stash_duration,0))) AS stash_avg_duration,\n",
    "MIN((IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),stash_duration,0))) AS stash_min_duration,\n",
    "SUM(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '1' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),clearedbalance,NULL))) td_balance,\n",
    "SUM(DISTINCT(IF(account_type LIKE '%Stash%' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),clearedbalance,NULL))) stash_balance,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '4' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),ofstandardaccountid,NULL))) td_accounts_completed_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%' AND td_status = '9' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)),ofstandardaccountid,NULL))) td_accounts_broken_cnt,\n",
    "COUNT(DISTINCT(IF(account_type <> 'Tonik Account', ofstandardaccountid,NULL))) deposit_accs_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Stash%',ofstandardaccountid,NULL))) stash_accounts_opened_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Time Deposit%',ofstandardaccountid,NULL))) td_accounts_opened_cnt,\n",
    "COUNT(DISTINCT(IF(account_type LIKE '%Stash%' and closed = 'Y' and (DATE(ofdateclosed) = '1970-01-01' OR DATE(ofdateclosed) < DATE(ln_snapshot_date)) ,ofstandardaccountid,NULL))) stash_accounts_closed_cnt,\n",
    "FROM \n",
    "deposit_acc_main\n",
    "where\n",
    "DATE(reccreatedon) < DATE(ln_snapshot_date) and account_type <> 'Tonik Account'\n",
    "GROUP BY 1,2\n",
    ")\n",
    "\n",
    "SELECT DISTINCT \n",
    "deposit_acc_main.customer_id,\n",
    "deposit_acc_main.ln_snapshot_date,\n",
    "td_balance,\n",
    "stash_balance,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "deposit_accs_cnt,\n",
    "stash_accounts_opened_cnt,\n",
    "stash_accounts_closed_cnt,\n",
    "td_accounts_opened_cnt,\n",
    "td_accounts_completed_cnt,\n",
    "td_accounts_broken_cnt,\n",
    "med_days_bw_td_tsa_acct_open,\n",
    "med_days_bw_new_dep_acct_open,\n",
    "med_days_bw_td_acct_open,\n",
    "td_max_duration,\n",
    "td_min_duration,\n",
    "td_avg_duration,\n",
    "stash_max_duration,\n",
    "stash_avg_duration,\n",
    "stash_min_duration\n",
    "\n",
    "FROM (SELECT DISTINCT customer_id,ln_snapshot_date FROM deposit_acc_main) deposit_acc_main  \n",
    "LEFT JOIN deposit_account_counts a ON deposit_acc_main.customer_id = a.customer_id AND a.ln_snapshot_date =deposit_acc_main.ln_snapshot_date\n",
    "LEFT JOIN dep_days_bt_trans_med b ON b.customer_id = deposit_acc_main.customer_id AND b.ln_snapshot_date =deposit_acc_main.ln_snapshot_date\n",
    "ORDER BY 2 DESC\n",
    ")\n",
    "SELECT\n",
    "COALESCE(first_product_data.first_product,'Unknown') as tx_first_product,\n",
    "COALESCE(first_product_data.first_product_user_segment,'Ghost Users') tx_first_product_user_segment,\n",
    "acc.customerid as customer_id,\n",
    "first_loan_disb_appln_date,\n",
    "onboarding_date,\n",
    "ln_user_type,\n",
    "acc.ln_snapshot_date,\n",
    "ln_mature_fpd30_flag,\n",
    "ln_fpd30_flag,\n",
    "ln_mature_fspd30_flag,\n",
    "ln_fspd30_flag,\n",
    "ln_self_dec_income,\n",
    "ln_marital_status,\n",
    "ln_education_level,\n",
    "ln_nature_of_work_new,\n",
    "onb_email_verified_flag,\n",
    "onb_place_of_birth,\n",
    "onb_doc_type,\n",
    "onb_country,\n",
    "onb_province,\n",
    "onb_city,\n",
    "onb_barangay,\n",
    "onb_postalcode,\n",
    "onb_latitude,\n",
    "onb_longitude,\n",
    "onb_osversion,\n",
    "onb_kyc_status,\n",
    "onb_kyc_status_upgrade_datetime,\n",
    "ln_osversion,\n",
    "ln_brand,\n",
    "ln_cnt_dependents,\n",
    "ln_source_funds_new,\n",
    "ln_employment_type_new,\n",
    "ln_industry_new,\n",
    "ln_company_name,\n",
    "ln_salary_scaled_income,\n",
    "ln_vas_opted_flag,\n",
    "ln_age,\n",
    "ln_mobile_no,\n",
    "ln_alt_mobile_no,\n",
    "ln_province,\n",
    "ln_city,\n",
    "ln_barangay,\n",
    "ln_latitude,\n",
    "ln_longitude,\n",
    "ln_doc_type,\n",
    "ln_ref1_type,\n",
    "onb_first_name,\n",
    "onb_middle_name,\n",
    "onb_last_name,\n",
    "onb_age,\n",
    "onb_gender,\n",
    "onb_mobile_no,\n",
    "onb_email,\n",
    "ln_loan_applied_flag,\n",
    "ln_facta_flag,\n",
    "ln_dl_rule_reject_flag,\n",
    "ln_taran_rule_reject_flag,\n",
    "ln_taran_scorecard_reject_flag,\n",
    "ln_cdd_reject_flag,\n",
    "ln_marked_underwriter_check_flag,\n",
    "ln_underwriting_reject_flag,\n",
    "ln_vas_used_flag,\n",
    "ln_os_type,\n",
    "ln_address,\n",
    "ln_postal_code,\n",
    "ln_doc_number,\n",
    "ln_source_funds,\n",
    "ln_employment_type,\n",
    "ln_nature_of_work,\n",
    "ln_industry,\n",
    "ln_ref2_type,\n",
    "onb_self_dec_income,\n",
    "onb_company_name,\n",
    "credo_inquiry_date,\n",
    "    cust_status_flag,\n",
    "    cust_status_close_date,\n",
    "\tdob_observation_date,\n",
    "\tdays_since_credo_call_onb,\n",
    "\tdays_since_credo_call_loan_application,\n",
    "    first_applied_loan_appln_time,\n",
    "    first_applied_loan_type,\n",
    "    first_applied_loan_tenor,\n",
    "    first_applied_loan_amount,\n",
    "    first_applied_product_type,\n",
    "    first_disb_loan_appln_time,\n",
    "    first_disb_loan_type,\n",
    "    first_disb_loan_tenor,\n",
    "    first_disb_loan_amount,\n",
    "    first_disb_product_type,\n",
    "    first_disb_loan_disb_time,\n",
    "    last_applied_loan_appln_time,\n",
    "    last_applied_loan_decision,\n",
    "    last_applied_os_type,\n",
    "    last_applied_loan_type,\n",
    "    last_applied_loan_tenor,\n",
    "    last_applied_loan_amount,\n",
    "    last_applied_product_type,\n",
    "    last_applied_crif_id,\n",
    "    last_applied_cic_called_flag,\n",
    "    last_applied_cic_hit_flag,\n",
    "    last_applied_cic_score,\n",
    "    last_applied_credo_ref_no,\n",
    "    last_applied_credo_score,\n",
    "    last_applied_demo_score,\n",
    "    last_applied_apps_score,\n",
    "    last_disb_loan_appln_time,\n",
    "    last_disb_loan_disb_time,\n",
    "    last_disb_loan_type,\n",
    "    last_disb_loan_tenor,\n",
    "    last_disb_loan_amount,\n",
    "    last_disb_crif_id,\n",
    "    last_disb_product_type,\n",
    "    \n",
    "EXTRACT(YEAR FROM onboarding_date) onb_year,\n",
    "EXTRACT(MONTH FROM onboarding_date) onb_month_of_year,\n",
    "EXTRACT(WEEK FROM onboarding_date) - EXTRACT(WEEK FROM DATE_TRUNC(onboarding_date, MONTH)) + 1 AS onb_week_of_month,\n",
    "--EXTRACT(WEEK FROM onboarding_date) onboarding_week,\n",
    "EXTRACT(DAY FROM onboarding_date) onb_day_of_month,\n",
    "EXTRACT(TIME FROM onboarding_date) onb_time_of_day,\n",
    "a.tx_cnt_cash_in_total,\n",
    "a.tx_cnt_cash_in_ob2t,\n",
    "a.tx_cnt_cash_in_ot2t,\n",
    "a.tx_amt_cash_in_total,\n",
    "a.tx_amt_cash_in_ob2t,\n",
    "a.tx_amt_cash_in_ot2t,\n",
    "a.tx_cnt_cash_out_total,\n",
    "a.tx_cnt_cash_out_billpay,\n",
    "a.tx_cnt_cash_out_cards,\n",
    "a.tx_cnt_cash_out_t2ot,\n",
    "a.tx_cnt_cash_out_t2ob,\n",
    "a.tx_amt_cash_out_total,\n",
    "a.tx_amt_cash_out_billpay,\n",
    "a.tx_amt_cash_out_cards,\n",
    "a.tx_amt_cash_out_t2ot,\n",
    "a.tx_amt_cash_out_t2ob,\n",
    "overall_avg_days_bt_trans tx_avg_days_bt_trans,\n",
    "net_cash_in_avg_days_bt_trans tx_avg_days_bt_cash_in_trans,\n",
    "net_cash_out_avg_days_bt_trans tx_avg_days_bt_cash_out_trans,\n",
    "overall_med_days_bt_trans tx_med_days_bt_trans,\n",
    "cash_in_med_days_bt_trans tx_med_days_bt_cash_in_trans,\n",
    "cash_out_med_days_bt_trans tx_med_days_bt_cash_out_trans,\n",
    "deposit_accs_cnt tx_deposit_accnt_cnt,\n",
    "stash_accounts_opened_cnt tx_stash_accnt_opened_cnt,\n",
    "stash_accounts_closed_cnt tx_stash_accnt_closed_cnt,\n",
    "stash_balance tx_stash_balance,\n",
    "td_accounts_opened_cnt tx_td_accnt_opened_cnt,\n",
    "td_accounts_completed_cnt tx_td_accnt_completed_cnt,\n",
    "td_accounts_broken_cnt tx_td_accnt_broken_cnt,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "td_balance tx_td_balance,\n",
    "td_max_duration tx_td_max_duration,\n",
    "td_min_duration tx_td_min_duration,\n",
    "td_avg_duration tx_td_avg_duration,\n",
    "stash_max_duration tx_stash_max_duration,\n",
    "stash_avg_duration tx_stash_avg_duration,\n",
    "stash_min_duration tx_stash_min_duration,\n",
    "med_days_bw_td_tsa_acct_open tx_med_days_bw_td_tsa_acct_open,\n",
    "med_days_bw_new_dep_acct_open tx_med_days_bw_new_dep_acct_open,\n",
    "med_days_bw_td_acct_open tx_med_days_bw_td_acct_open,\n",
    "tx_cnt_completed_loan_apps tx_cnt_completed_loans,\n",
    "tx_cnt_rejected_loan_apps tx_cnt_rejected_loans,\n",
    "tx_cnt_active_loan_apps tx_cnt_active_loans,\n",
    "tx_cnt_applied_loan_apps tx_cnt_applied_loan,\n",
    "tx_cnt_approved_loan_apps tx_cnt_approved_loans,\n",
    "tx_cnt_disbursed_loan_apps tx_cnt_disbursed_loans,\n",
    "tx_incomplete_loan_apps tx_cnt_incomplete_loan_apps,\n",
    "COALESCE(tx_min_age_completed_loans,0) tx_min_age_completed_loans,\n",
    "COALESCE(tx_max_age_completed_loans,0) tx_max_age_completed_loans,\n",
    "COALESCE(tx_avg_age_completed_loans,0) tx_avg_age_completed_loans,\n",
    "cnt_installments_paid tx_cnt_installments_paid_tot,\n",
    "tx_cnt_installments_paid_tot_with_dpd,\n",
    "tx_amt_installments_paid_tot_with_dpd,\n",
    "tx_total_due_amount tx_amount_tot_due,\n",
    "total_amt_installments_paid tx_amt_installments_paid_tot,\n",
    "--tx_cnt_installments_paid_last_disb_withdpd,\n",
    "--total_amt_installments_paid_last_disb/last_disb_loan_amount as ratio_amt_paid_last_disb_loan,\n",
    "--cnt_installments_paid_last_disb tx_cnt_installments_paid_last_disb,\n",
    "--total_amt_installments_paid_last_disb tx_amt_installments_paid_last_disb,\n",
    "cnt_fpd10 tx_cnt_fpd10_ever,\n",
    "cnt_fpd30 tx_cnt_fpd30_ever,\n",
    "cnt_fspd30 tx_cnt_fspd30_ever,\n",
    "cnt_dpd_gt_1 tx_cnt_dpd_gt_1_ever,\n",
    "cnt_dpd_gt_5 tx_cnt_dpd_gt_5_ever,\n",
    "\n",
    "loan_metrics.max_ever_dpd tx_max_ever_dpd,\n",
    "/*max_ever_dpd_30d tx_max_dpd_30d,\n",
    "max_ever_dpd_60d tx_max_dpd_60d,\n",
    "max_ever_dpd_90d tx_max_dpd_90d,\n",
    "max_ever_dpd_120d tx_max_dpd_120d,\n",
    "max_ever_dpd_150d tx_max_dpd_150d,\n",
    "max_ever_dpd_180d tx_max_dpd_180d,\n",
    "max_current_dpd tx_max_current_dpd,*/\n",
    "ln_any_prev_disb_loan_sil_mobile_flag\n",
    "FROM input_customers acc\n",
    "LEFT JOIN (SELECT DISTINCT cust_id,onboarding_date,ln_snapshot_date from cust_onboarding_acc_data) cust_onboarding_acc_data on cust_onboarding_acc_data.cust_id = acc.customerid --and acc.ln_snapshot_date = cust_onboarding_acc_data.ln_snapshot_date\n",
    "LEFT JOIN transactions_final a ON acc.customerid = a.customer_id --and acc.ln_snapshot_date = a.ln_snapshot_date\n",
    "LEFT JOIN days_bt_trans_avg b ON acc.customerid = b.customer_id --and b.ln_snapshot_date = acc.ln_snapshot_date\n",
    "LEFT JOIN days_bt_trans_med c ON acc.customerid = c.customer_id --and c.ln_snapshot_date = acc.ln_snapshot_date\n",
    "LEFT JOIN complete_deposit_metrics d on d.customer_id = acc.customerid-- and acc.ln_snapshot_date = d.ln_snapshot_date\n",
    "LEFT JOIN loan_metrics ON cast(loan_metrics.customerid as string) = acc.customerid --and acc.ln_snapshot_date = loan_metrics.ln_snapshot_date\n",
    "LEFT JOIN first_product_data ON first_product_data.customer_id = acc.customerid and acc.ln_snapshot_date = first_product_data.ln_snapshot_date\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab96e3",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.b_score_snapshot_customer_event_data_20250720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "CREATE OR REPLACE TABLE worktable_data_analysis.b_score_snapshot_customer_event_data_20250720 as\n",
    "with input_customers as ( \n",
    "select * from `worktable_data_analysis.b_score_snapshot_customer_data_20250720`\n",
    "),\n",
    "af_link AS\n",
    "(\n",
    "  ## To get the AF ID and Customer ID Link (using the first install of a customer)\n",
    "  SELECT DISTINCT appsflyer_id, customer_user_id, install_time\n",
    "  FROM `appsflyer_raw.in_app_events_report` in_apps_events\n",
    "  JOIN `dl_customers_db_raw.tdbk_customer_mtb` c ON c.cust_id = in_apps_events.customer_user_id\n",
    "  JOIN input_customers ON In_apps_events.customer_user_id = input_customers.customerid and DATE(in_apps_events._partitiondate) < DATE(input_customers.ln_snapshot_date)\n",
    "  WHERE 1=1\n",
    "  AND customer_user_id IS NOT NULL\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_user_id ORDER BY install_time ASC) = 1\n",
    " \n",
    "  UNION ALL\n",
    " \n",
    "  SELECT DISTINCT appsflyer_id, customer_user_id, install_time\n",
    "  FROM `appsflyer_raw.organic_in_app_events_report` organic_in_apps_events\n",
    "  JOIN `dl_customers_db_raw.tdbk_customer_mtb` c ON c.cust_id = organic_in_apps_events.customer_user_id\n",
    " JOIN input_customers ON organic_in_apps_events.customer_user_id = input_customers.customerid and DATE(organic_in_apps_events._partitiondate) < DATE(input_customers.ln_snapshot_date)\n",
    "  WHERE 1=1\n",
    "  AND customer_user_id IS NOT NULL\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_user_id ORDER BY install_time ASC) = 1\n",
    ")\n",
    ", events AS\n",
    "(\n",
    "  SELECT DISTINCT\n",
    "  cust_id as customer_id,\n",
    "  ln_snapshot_date,\n",
    "COUNT(DISTINCT IF(event_name = 'App_Launch',event_uuid,NULL)) meng_no_of_logins,\n",
    "COUNT(DISTINCT IF(\n",
    "    event_name like any ('Loans_%_Calculator'),moengagerefid,NULL)) meng_calculator_count,\n",
    "COUNT(DISTINCT IF(\n",
    "    event_name like any ('Loans_%_Calculator'),event_uuid,NULL)) meng_calculator_tot_visit_cnt\n",
    "  FROM `dl_customers_db_raw.tdbk_customer_mtb` c\n",
    "  JOIN `moengage_raw.events_hourly` a ON c.cust_id = cast(a.customer_id as string)\n",
    " JOIN input_customers ON cast(a.customer_id as string) = input_customers.customerid \n",
    "  WHERE 1=1 and DATE(event_time) < DATE(input_customers.ln_snapshot_date)\n",
    "  --AND event_name like any ('%App_Launch%','Loans_%_Calculator')\n",
    "  group by 1,2\n",
    "  ),\n",
    "\n",
    "campaign_data as\n",
    "(  SELECT DISTINCT a.customer_user_id, a.appsflyer_id, a.media_source, a.partner, a.campaign, a.Retargeting_Conversion_Type, \n",
    "  CASE\n",
    "  WHEN a.media_source = 'website_channel=website_ss_ui=true_ss_gtm_ui=true_ss_qr=c' THEN 'Website'\n",
    "  WHEN a.media_source IS NULL AND a.partner IS NULL AND a.campaign IS NULL THEN 'Organic'\n",
    "  ELSE b.source END source,\n",
    "CASE\n",
    "  WHEN a.media_source = 'website_channel=website_ss_ui=true_ss_gtm_ui=true_ss_qr=c' THEN 'Website'\n",
    "  WHEN a.media_source IS NULL AND a.partner IS NULL AND a.campaign IS NULL THEN 'Organic'\n",
    "  ELSE b.source_group END source_group,\n",
    "  FROM \n",
    "  (\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    ln_snapshot_date,\n",
    "    'Install' Retargeting_Conversion_Type,\n",
    "    FROM `appsflyer_raw.in_app_events_report` organic_in_apps_events\n",
    "    JOIN input_customers ON organic_in_apps_events.customer_user_id = input_customers.customerid and DATE(organic_in_apps_events._partitiondate) < DATE(input_customers.ln_snapshot_date)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    ln_snapshot_date,\n",
    "    'Install' Retargeting_Conversion_Type\n",
    "    FROM `appsflyer_raw.organic_in_app_events_report` organic_in_apps_events\n",
    "    JOIN input_customers ON organic_in_apps_events.customer_user_id = input_customers.customerid and DATE(organic_in_apps_events._partitiondate) < DATE(input_customers.ln_snapshot_date)\n",
    "\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT DISTINCT\n",
    "    install_time,\n",
    "    customer_user_id,\n",
    "    AppsFlyer_ID,\n",
    "    media_source,\n",
    "    partner,\n",
    "    campaign,\n",
    "    ln_snapshot_date,\n",
    "    Retargeting_Conversion_Type\n",
    "    FROM `appsflyer_raw.in_app_events_retarget` organic_in_apps_events\n",
    "    JOIN input_customers ON organic_in_apps_events.customer_user_id = input_customers.customerid and DATE(organic_in_apps_events._partitiondate) < DATE(input_customers.ln_snapshot_date)\n",
    "  ) a\n",
    "  LEFT JOIN `prj-prod-dataplatform.worktable_datachampions.installs_attribution_mapping` b\n",
    "  ON \n",
    "    COALESCE(a.media_source,a.partner,a.campaign) = COALESCE(b.media_source,b.partner,b.campaign)\n",
    "\n",
    "  WHERE 1=1\n",
    " \n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY a.customer_user_id ORDER BY install_time DESC) = 1\n",
    "  ORDER BY customer_user_id\n",
    " ),\n",
    "final_output as (\n",
    "SELECT DISTINCT\n",
    "input_customers.customerId,\n",
    "ln_snapshot_date,\n",
    "TIMESTAMP_DIFF(MIN(cust_mtb.created_dt),MIN(install_time),MINUTE) appsflyer_install_to_registration_minutes,\n",
    "FROM input_customers\n",
    "JOIN `dl_customers_db_raw.tdbk_customer_mtb` cust_mtb ON cust_mtb.cust_id = input_customers.customerId\n",
    "LEFT JOIN af_link b ON b.customer_user_id = input_customers.customerId\n",
    "group by 1,2\n",
    ")\n",
    "SELECT \n",
    "final_output.*,\n",
    "CASE WHEN campaign_data.source_group = 'Organic' THEN 'Organic'\n",
    "ELSE 'InOrganic' END AS channel_source_group,\n",
    "source as marketing_source_name,\n",
    "meng_no_of_logins,\n",
    "meng_calculator_count,\n",
    "meng_calculator_tot_visit_cnt,\n",
    "from final_output\n",
    "LEFT JOIN campaign_data ON campaign_data.customer_user_id = final_output.customerId\n",
    "LEFT JOIN events c ON c.customer_id = final_output.customerId and final_output.ln_snapshot_date = c.ln_snapshot_date\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18614d",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.b_score_snapshot_contactability_20250721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "CREATE OR REPLACE TABLE\n",
    "  `worktable_data_analysis.b_score_snapshot_contactability_20250721` AS\n",
    "with input_customers as ( \n",
    "select * from `worktable_data_analysis.b_score_snapshot_customer_data_20250721`),\n",
    "cust_emails as (\n",
    "  with temp_output as (\n",
    "select too.email as to_email, emailTranscript.from.email as from_email,\n",
    "CASE WHEN SPLIT(emailTranscript.from.email,'@')[SAFE_OFFSET(1)] like '%tonik%' THEN 'outbound'\n",
    "WHEN SPLIT(too.email,'@')[SAFE_OFFSET(1)] like '%tonik%' THEN 'inbound'\n",
    "ELSE 'outbound'\n",
    "END AS category,\n",
    "creationTime\n",
    "from `genesys_raw.emails`,\n",
    "unnest(emailTranscript) emailTranscript,unnest(emailTranscript.to) too\n",
    ")\n",
    "SELECT\n",
    "*,\n",
    "CASE WHEN category = 'outbound' THEN to_email\n",
    "WHEN category = 'inbound'THEN from_email\n",
    "WHEN from_email not like '%tonik%' THEN from_email\n",
    "END AS customer_email_address,\n",
    "FROM temp_output\n",
    "),\n",
    "cust_mobile_genesys_data as (\n",
    "SELECT\n",
    "input_customers.customerid,\n",
    "MAX(\n",
    "    CASE\n",
    "      WHEN DATE(call_history.callDatetime) between DATE_SUB(input_customers.ln_snapshot_date, INTERVAL 90 DAY) and DATE(input_customers.ln_snapshot_date) THEN 1\n",
    "      ELSE 0\n",
    "  END\n",
    "    ) AS flag_contactable_last90D,\n",
    "  count(\n",
    "    CASE\n",
    "      WHEN DATE(call_history.callDatetime) between DATE_SUB(input_customers.ln_snapshot_date, INTERVAL 90 DAY) and DATE(input_customers.ln_snapshot_date) THEN 1\n",
    "      ELSE null\n",
    "  END\n",
    "    ) AS count_contactable_last90D,\n",
    "  MAX(\n",
    "  CASE\n",
    "    WHEN DATE(call_history.callDatetime) between DATE_SUB(input_customers.ln_snapshot_date, INTERVAL 90 DAY) and DATE(input_customers.ln_snapshot_date)\n",
    "  AND COALESCE(call_history.campaignName, 'NULL') IN ('UPSELL VERIFICATION CALLOUT',\n",
    "    'UPSELL VERIFICATION_1st interval',\n",
    "    'UPSELL VERIFICATION_2nd interval',\n",
    "    'UPSELL VERIFICATION_3rd interval')\n",
    "  AND genesysWrapupDisposition = 'UW - CLIENT ANSWERED'\n",
    "  AND talkTime > 12 THEN 1\n",
    "    ELSE 0\n",
    "END\n",
    "  ) AS flag_contactable_upsell_last90D,\n",
    "  COUNT(\n",
    "  CASE\n",
    "    WHEN call_history.callDatetime between DATETIME_SUB(input_customers.ln_snapshot_date, INTERVAL 90 DAY) and input_customers.ln_snapshot_date\n",
    "  AND COALESCE(call_history.campaignName, 'NULL') IN ('UPSELL VERIFICATION CALLOUT',\n",
    "    'UPSELL VERIFICATION_1st interval',\n",
    "    'UPSELL VERIFICATION_2nd interval',\n",
    "    'UPSELL VERIFICATION_3rd interval')\n",
    "  AND genesysWrapupDisposition = 'UW - CLIENT ANSWERED'\n",
    "  AND talkTime > 12 THEN 1\n",
    "    ELSE NULL\n",
    "END\n",
    "  ) AS count_contactable_upsell_last90D,\n",
    "COUNT(CASE WHEN calldirection = 'outbound' and allcampaignName IN ('SIP_Reminder','Prod_Reminder','Special_Reminder','Tonik Agentless -Collection','Prod_Reminder_IVR','Agentless-Collection_UATtest_071122','COLLECTION REMINDER_B1B2','COLLECTION REMINDER_C1C2','Reminder_Voicebot_Calling_F','Reminder_Voicebot_Calling_M','Enhanced_Reminder_Voicebot_Calling_F','Enhanced_Reminder_Voicebot_Calling_M','COLLECTION_REMINDER (B1B2)','COLLECTION_REMINDER (A1A2)','COLLECTION_REMINDER (C1C2)','Special_Reminder Collection','Prod_Soft Collections (1-30DPD)', 'Special_Soft Collections (1-30DPD)', 'PROD_SOFT_Collection 1-30 DPD_1','PRODSPECIAL_B1_PROJECTNORM','BP_Soft Collection (1-30DPD)','Prod_Soft Collections (31-60DPD)', 'Special_Soft Collections (31-60DPD)', 'Prod_Soft Collections 31-60DPD','BP_Soft Collections (31-60DPD)','L3M No Payment_Mid Range Collections', 'L3M With Payment_Mid Range Collections', 'PTPr BPs_Mid Range Collections',\n",
    "         'Prod_Mid Range Collections', 'Special_Mid Range Collections','BP_Mid Range Collections','PROD_MIDRANGE','PROD_SOFT_Collection 1-60DPD','PROD_SOFT_Collection 1-60 DPD','PROD_Ref Persons-Daily') THEN 1 ELSE null END) AS outbound_call_count_upsell_mkt,\n",
    "COUNT(CASE WHEN calldirection = 'inbound' and allcampaignName IN ('SIP_Reminder','Prod_Reminder','Special_Reminder','Tonik Agentless -Collection','Prod_Reminder_IVR','Agentless-Collection_UATtest_071122','COLLECTION REMINDER_B1B2','COLLECTION REMINDER_C1C2','Reminder_Voicebot_Calling_F','Reminder_Voicebot_Calling_M','Enhanced_Reminder_Voicebot_Calling_F','Enhanced_Reminder_Voicebot_Calling_M','COLLECTION_REMINDER (B1B2)','COLLECTION_REMINDER (A1A2)','COLLECTION_REMINDER (C1C2)','Special_Reminder Collection','Prod_Soft Collections (1-30DPD)', 'Special_Soft Collections (1-30DPD)', 'PROD_SOFT_Collection 1-30 DPD_1','PRODSPECIAL_B1_PROJECTNORM','BP_Soft Collection (1-30DPD)','Prod_Soft Collections (31-60DPD)', 'Special_Soft Collections (31-60DPD)', 'Prod_Soft Collections 31-60DPD','BP_Soft Collections (31-60DPD)','L3M No Payment_Mid Range Collections', 'L3M With Payment_Mid Range Collections', 'PTPr BPs_Mid Range Collections',\n",
    "         'Prod_Mid Range Collections', 'Special_Mid Range Collections','BP_Mid Range Collections','PROD_MIDRANGE','PROD_SOFT_Collection 1-60DPD','PROD_SOFT_Collection 1-60 DPD','PROD_Ref Persons-Daily')  THEN 1 ELSE null END) AS inbound_call_count_upsell_mkt,\n",
    "COUNT(CASE WHEN calldirection = 'outbound' and COALESCE(allcampaignName,'#####') NOT IN ('SIP_Reminder','Prod_Reminder','Special_Reminder','Tonik Agentless -Collection','Prod_Reminder_IVR','Agentless-Collection_UATtest_071122','COLLECTION REMINDER_B1B2','COLLECTION REMINDER_C1C2','Reminder_Voicebot_Calling_F','Reminder_Voicebot_Calling_M','Enhanced_Reminder_Voicebot_Calling_F','Enhanced_Reminder_Voicebot_Calling_M','COLLECTION_REMINDER (B1B2)','COLLECTION_REMINDER (A1A2)','COLLECTION_REMINDER (C1C2)','Special_Reminder Collection','Prod_Soft Collections (1-30DPD)', 'Special_Soft Collections (1-30DPD)', 'PROD_SOFT_Collection 1-30 DPD_1','PRODSPECIAL_B1_PROJECTNORM','BP_Soft Collection (1-30DPD)','Prod_Soft Collections (31-60DPD)', 'Special_Soft Collections (31-60DPD)', 'Prod_Soft Collections 31-60DPD','BP_Soft Collections (31-60DPD)','L3M No Payment_Mid Range Collections', 'L3M With Payment_Mid Range Collections', 'PTPr BPs_Mid Range Collections',\n",
    "         'Prod_Mid Range Collections', 'Special_Mid Range Collections','BP_Mid Range Collections','PROD_MIDRANGE','PROD_SOFT_Collection 1-60DPD','PROD_SOFT_Collection 1-60 DPD','PROD_Ref Persons-Daily') THEN 1 ELSE null END) AS outbound_call_count_others,\n",
    "COUNT(CASE WHEN calldirection = 'inbound' and COALESCE(allcampaignName,'#####') NOT IN ('SIP_Reminder','Prod_Reminder','Special_Reminder','Tonik Agentless -Collection','Prod_Reminder_IVR','Agentless-Collection_UATtest_071122','COLLECTION REMINDER_B1B2','COLLECTION REMINDER_C1C2','Reminder_Voicebot_Calling_F','Reminder_Voicebot_Calling_M','Enhanced_Reminder_Voicebot_Calling_F','Enhanced_Reminder_Voicebot_Calling_M','COLLECTION_REMINDER (B1B2)','COLLECTION_REMINDER (A1A2)','COLLECTION_REMINDER (C1C2)','Special_Reminder Collection','Prod_Soft Collections (1-30DPD)', 'Special_Soft Collections (1-30DPD)', 'PROD_SOFT_Collection 1-30 DPD_1','PRODSPECIAL_B1_PROJECTNORM','BP_Soft Collection (1-30DPD)','Prod_Soft Collections (31-60DPD)', 'Special_Soft Collections (31-60DPD)', 'Prod_Soft Collections 31-60DPD','BP_Soft Collections (31-60DPD)','L3M No Payment_Mid Range Collections', 'L3M With Payment_Mid Range Collections', 'PTPr BPs_Mid Range Collections',\n",
    "         'Prod_Mid Range Collections', 'Special_Mid Range Collections','BP_Mid Range Collections','PROD_MIDRANGE','PROD_SOFT_Collection 1-60DPD','PROD_SOFT_Collection 1-60 DPD','PROD_Ref Persons-Daily')  THEN 1 ELSE null END) AS inbound_call_count_others,\n",
    "\n",
    "\n",
    "FROM input_customers\n",
    "LEFT JOIN (SELECT DISTINCT cust_id,mobile_no, valid_from_dt FROM `datalake_worktables.customer_mobile_mail_dtls`\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY cust_id,mobile_no order by valid_from_dt asc) = 1\n",
    ") mobile_details on input_customers.customerid = mobile_details.cust_id and DATE(mobile_details.valid_from_dt) < DATE(ln_snapshot_date)\n",
    "LEFT JOIN `risk_credit_mis.call_attempt_history_gensys` call_history ON RIGHT(mobile_details.mobile_no,10) = right(call_history.mobileNumber,10) and DATE(call_history.callDatetime) < DATE(input_customers.ln_snapshot_date) and connected = 1 and mediaType = 'voice'\n",
    "GROUP BY 1\n",
    "),\n",
    "cust_email_genesys_data as (\n",
    "SELECT\n",
    "input_customers.customerid,\n",
    "COUNT(CASE WHEN category = 'outbound' THEN 1 ELSE null END) AS outbound_email_count,\n",
    "COUNT(CASE WHEN category = 'inbound'  THEN 1 ELSE null END) AS inbound_email_count\n",
    "FROM input_customers\n",
    "LEFT JOIN (SELECT DISTINCT cust_id,email, valid_from_dt FROM `datalake_worktables.customer_mobile_mail_dtls`\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY cust_id,email order by valid_from_dt asc) = 1\n",
    ") email_details on input_customers.customerid = email_details.cust_id and DATE(email_details.valid_from_dt) < DATE(ln_snapshot_date)\n",
    "left JOIN cust_emails ON email_details.email = cust_emails.customer_email_address and DATE(cust_emails.creationTime) < DATE(input_customers.ln_snapshot_date)\n",
    "GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "  input_customers.customerid,\n",
    "  input_customers.ln_snapshot_date,\n",
    " flag_contactable_last90D,\n",
    " count_contactable_last90D,\n",
    " count_contactable_upsell_last90D,\n",
    " flag_contactable_upsell_last90D,\n",
    "inbound_call_count_upsell_mkt,\n",
    "outbound_call_count_upsell_mkt,\n",
    "inbound_call_count_others,\n",
    "outbound_call_count_others,\n",
    "outbound_email_count,\n",
    "inbound_email_count\n",
    "\n",
    "\n",
    "FROM input_customers\n",
    "LEFT JOIN cust_mobile_genesys_data on cust_mobile_genesys_data.customerid = CAST(input_customers.customerid AS STRING) --and cust_mobile_genesys_data.ln_snapshot_date = input_customers.ln_snapshot_date\n",
    "LEFT JOIN cust_email_genesys_data ON  cust_email_genesys_data.customerid = CAST(input_customers.customerid AS STRING)\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079f983",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.b_score_snapshot_combined_data_20250807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"  \n",
    "CREATE OR REPLACE TABLE `worktable_data_analysis.b_score_snapshot_combined_data_20250807` as\n",
    "with cust_doc_data as (\n",
    "SELECT \n",
    "custid,\n",
    "case when docExpiryDate = \"NA\" then '9999-12-31'\n",
    "          when docExpiryDate is null then '9999-12-31'\n",
    "          else PARSE_DATE('%e %b %Y', docExpiryDate)\n",
    "        end as docExpiryDate\n",
    "FROM dl_loans_db_raw.tdbk_loan_customer_details\n",
    "),\n",
    "jira_data as (\n",
    "SELECT customer_id,\n",
    "ln_snapshot_date,\n",
    "count(*) as cnt_jira_tickets_created,\n",
    " from \n",
    "worktable_data_analysis.b_score_snapshot_customer_data_20250807 a \n",
    "JOIN `jira_raw.cc_tickets` b on a.customerid = b.customer_id and b.created_date < a.ln_snapshot_date\n",
    "group by 1,2\n",
    "),\n",
    "cust_email as (\n",
    "SELECT cust_id,email as ln_email, valid_from_dt,ln_snapshot_date FROM `datalake_worktables.customer_mobile_mail_dtls` email_details \n",
    "join worktable_data_analysis.b_score_snapshot_customer_data_20250807 input_customers on input_customers.customerid = email_details.cust_id and DATETIME(email_details.valid_from_dt) < DATETIME(ln_snapshot_date)\n",
    "QUALIFY ROW_NUMBER() OVER(PARTITION BY cust_id,ln_snapshot_date order by valid_from_dt desc) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "acc.customer_id,\n",
    "dob as birth_date,\n",
    "first_loan_disb_appln_date,\n",
    "onboarding_date,\n",
    "tx_first_product,\n",
    "tx_first_product_user_segment,\n",
    "credo_inquiry_date,\n",
    "cust_status_flag,\n",
    "cust_status_close_date,\n",
    "days_since_credo_call_onb,\n",
    "days_since_credo_call_loan_application,\n",
    "--ln_prod_type ln_product_type,\n",
    "--ln_repeat_loan_type,\n",
    "acc.ln_snapshot_date,\n",
    "--ln_disb_dtime ln_loan_disb_time,\n",
    "--ln_user_type,\n",
    "--ln_loan_type,\n",
    "onb_doc_type,\n",
    "ln_osversion,\n",
    "onb_kyc_status,\n",
    "onb_email_verified_flag,\n",
    "onb_place_of_birth,\n",
    "onb_country,\n",
    "onb_province,\n",
    "onb_city,\n",
    "onb_barangay,\n",
    "onb_postalcode,\n",
    "onb_latitude,\n",
    "onb_longitude,\n",
    "onb_osversion,\n",
    "onb_first_name,\n",
    "onb_middle_name,\n",
    "onb_last_name,\n",
    "onb_age,\n",
    "onb_gender,\n",
    "onb_mobile_no,\n",
    "onb_email,\n",
    "ln_email,\n",
    "onb_self_dec_income,\n",
    "onb_company_name,\n",
    "onb_kyc_status_upgrade_datetime,\n",
    "dob_observation_date,\n",
    "ln_brand,\n",
    "ln_cnt_dependents,\n",
    "ln_source_funds_new,\n",
    "ln_employment_type_new,\n",
    "ln_industry_new,\n",
    "ln_company_name,\n",
    "ln_salary_scaled_income,\n",
    "ln_self_dec_income,\n",
    "ln_marital_status,\n",
    "ln_education_level,\n",
    "ln_nature_of_work_new,\n",
    "ln_vas_opted_flag,\n",
    "ln_age,\n",
    "ln_mobile_no,\n",
    "ln_alt_mobile_no,\n",
    "ln_province,\n",
    "ln_city,\n",
    "ln_barangay,\n",
    "ln_latitude,\n",
    "ln_longitude,\n",
    "ln_doc_type,\n",
    "ln_ref1_type,\n",
    "ln_ref2_type,\n",
    "ln_loan_applied_flag,\n",
    "ln_facta_flag,\n",
    "ln_dl_rule_reject_flag,\n",
    "ln_taran_rule_reject_flag,\n",
    "ln_taran_scorecard_reject_flag,\n",
    "ln_cdd_reject_flag,\n",
    "ln_marked_underwriter_check_flag,\n",
    "ln_underwriting_reject_flag,\n",
    "ln_vas_used_flag,\n",
    "ln_os_type,\n",
    "ln_address,\n",
    "ln_postal_code,\n",
    "ln_doc_number,\n",
    "ln_source_funds,\n",
    "ln_employment_type,\n",
    "ln_nature_of_work,\n",
    "ln_industry,\n",
    "ln_mature_fspd30_flag,\n",
    "ln_fspd30_flag,\n",
    "ln_mature_fpd30_flag,\n",
    "ln_fpd30_flag,\n",
    "first_applied_loan_appln_time,\n",
    "first_applied_loan_type,\n",
    "first_applied_product_type,\n",
    "first_applied_loan_amount,\n",
    "first_applied_loan_tenor,\n",
    "first_disb_loan_appln_time,\n",
    "first_disb_loan_disb_time,\n",
    "first_disb_loan_type,\n",
    "first_disb_product_type,\n",
    "first_disb_loan_amount,\n",
    "first_disb_loan_tenor,\n",
    "last_disb_loan_appln_time,\n",
    "last_disb_loan_disb_time,\n",
    "last_disb_loan_type,\n",
    "last_disb_product_type,\n",
    "last_disb_loan_amount,\n",
    "last_disb_loan_tenor,\n",
    "last_disb_crif_id,\n",
    "last_applied_loan_appln_time,\n",
    "last_applied_loan_decision,\n",
    "last_applied_loan_type,\n",
    "last_applied_product_type,\n",
    "last_applied_loan_amount,\n",
    "last_applied_loan_tenor,\n",
    "last_applied_os_type,\n",
    "last_applied_crif_id last_applied_digitalloanaccountId,\n",
    "last_applied_cic_called_flag,\n",
    "last_applied_cic_hit_flag,\n",
    "last_applied_crif_id,\n",
    "last_applied_cic_score,\n",
    "last_applied_credo_ref_no,\n",
    "last_applied_credo_score,\n",
    "last_applied_demo_score,\n",
    "last_applied_apps_score,\n",
    "onb_year,\n",
    "onb_month_of_year,\n",
    "onb_week_of_month,\n",
    "onb_day_of_month,\n",
    "onb_time_of_day,\n",
    "tx_cnt_cash_in_total,\n",
    "tx_cnt_cash_in_ob2t,\n",
    "tx_cnt_cash_in_ot2t,\n",
    "tx_amt_cash_in_total,\n",
    "tx_amt_cash_in_ob2t,\n",
    "tx_amt_cash_in_ot2t,\n",
    "tx_cnt_cash_out_total,\n",
    "tx_cnt_cash_out_billpay,\n",
    "tx_cnt_cash_out_cards,\n",
    "tx_cnt_cash_out_t2ot,\n",
    "tx_cnt_cash_out_t2ob,\n",
    "tx_amt_cash_out_total,\n",
    "tx_amt_cash_out_billpay,\n",
    "tx_amt_cash_out_cards,\n",
    "tx_amt_cash_out_t2ot,\n",
    "tx_amt_cash_out_t2ob,\n",
    "tx_deposit_accnt_cnt,\n",
    "tx_stash_accnt_opened_cnt,\n",
    "tx_stash_accnt_closed_cnt,\n",
    "tx_stash_balance,\n",
    "tx_td_accnt_opened_cnt,\n",
    "tx_td_accnt_completed_cnt,\n",
    "tx_td_accnt_broken_cnt,\n",
    "tx_td_auto_roll_over_enabled,\n",
    "tx_td_balance,\n",
    "tx_td_max_duration,\n",
    "tx_td_min_duration,\n",
    "tx_td_avg_duration,\n",
    "tx_stash_max_duration,\n",
    "tx_stash_avg_duration,\n",
    "tx_stash_min_duration,\n",
    "tx_med_days_bw_td_tsa_acct_open,\n",
    "tx_med_days_bw_new_dep_acct_open,\n",
    "tx_med_days_bw_td_acct_open,\n",
    "tx_avg_days_bt_trans,\n",
    "tx_avg_days_bt_cash_in_trans,\n",
    "tx_avg_days_bt_cash_out_trans,\n",
    "tx_med_days_bt_trans,\n",
    "tx_med_days_bt_cash_in_trans,\n",
    "tx_med_days_bt_cash_out_trans,\n",
    "tx_cnt_applied_loan,\n",
    "tx_cnt_rejected_loans,\n",
    "tx_cnt_approved_loans,\n",
    "tx_cnt_disbursed_loans,\n",
    "tx_cnt_completed_loans,\n",
    "tx_cnt_active_loans,\n",
    "tx_cnt_incomplete_loan_apps,\n",
    "tx_cnt_installments_paid_tot,\n",
    "tx_amt_installments_paid_tot,\n",
    "tx_cnt_installments_paid_tot_with_dpd,\n",
    "tx_max_ever_dpd,\n",
    "--tx_cnt_installments_paid_last_disb_withdpd,\n",
    "tx_amt_installments_paid_tot_with_dpd,\n",
    "tx_amount_tot_due tx_loan_amount_tot_due,\n",
    "--tx_cnt_installments_paid_last_disb,\n",
    "--tx_amt_installments_paid_last_disb,\n",
    "tx_min_age_completed_loans,\n",
    "tx_max_age_completed_loans,\n",
    "tx_avg_age_completed_loans,\n",
    "tx_cnt_dpd_gt_1_ever,\n",
    "tx_cnt_fpd10_ever,\n",
    "tx_cnt_fpd30_ever,\n",
    "tx_cnt_fspd30_ever,\n",
    "tx_cnt_dpd_gt_5_ever,\n",
    "/*tx_max_ever_dpd,\n",
    "tx_max_dpd_30d,\n",
    "tx_max_dpd_60d,\n",
    "tx_max_dpd_120d,\n",
    "tx_max_dpd_150d,\n",
    "tx_max_dpd_180d,\n",
    "tx_max_current_dpd,*/\n",
    "CASE WHEN tdbk_referral_code_mtb.cust_id is not null THEN 1\n",
    "ELSE 0 END AS onb_referral_flag,\n",
    "in_fraud_blacklist as ln_fraud_blacklist_flag,\n",
    "in_negative_location ln_negative_location_flag,\n",
    "flag_contactable_last90D cs_contactable_last_90d_flag,\n",
    "count_contactable_last90D cs_contactable_last_90d_cnt,\n",
    "flag_contactable_upsell_last90D cs_contactable_last_90d_upsell_flag,\n",
    "count_contactable_upsell_last90D cs_contactable_last_90d_upsell_cnt,\n",
    "cnt_jira_tickets_created,\n",
    "CASE WHEN cust_doc_data.docExpiryDate <> '9999-12-31' AND DATE_DIFF(Date(cust_doc_data.docExpiryDate), DATE(acc.ln_snapshot_date) , DAY) >= 75 THEN 1\n",
    "ELSE 0 END onb_valid_documents_flag,\n",
    "ln_any_prev_disb_loan_sil_mobile_flag,\n",
    "CASE WHEN appsflyer_install_to_registration_minutes < 0 THEN NULL\n",
    "ELSE appsflyer_install_to_registration_minutes END appsflyer_install_to_registration_minutes,\n",
    "meng_no_of_logins,\n",
    "meng_calculator_count,\n",
    "meng_calculator_tot_visit_cnt,\n",
    "channel_source_group,\n",
    "marketing_source_name,\n",
    "outbound_call_count_upsell_mkt cs_cnt_outbound_calls_upsell_mkt,\n",
    "inbound_call_count_upsell_mkt cs_cnt_inbound_calls_upsell_mkt,\n",
    "-- outbound_email_count_upsell_mkt cs_cnt_outbound_emails_upsell_mkt,\n",
    "-- inbound_email_count_upsell_mkt cs_cnt_inbound_emails_upsell_mkt,\n",
    "outbound_call_count_others cs_cnt_outbound_calls_others,\n",
    "inbound_call_count_others cs_cnt_inbound_calls_others,\n",
    "-- outbound_email_count_others cs_cnt_outbound_emails_others,\n",
    "-- inbound_email_count_others cs_cnt_inbound_emails_others,\n",
    "outbound_email_count,\n",
    "inbound_email_count\n",
    "FROM \n",
    "`worktable_data_analysis.b_score_snapshot_customer_transaction_data_20250807` acc\n",
    "LEFT JOIN `worktable_data_analysis.b_score_snapshot_customer_event_data_20250807` events on events.customerid = acc.customer_id --and events.ln_snapshot_date = acc.ln_snapshot_date\n",
    "LEFT JOIN (SELECT cust_id,member_type,referral_type FROM dl_customers_db_raw.tdbk_referral_code_mtb\n",
    "WHERE tdbk_referral_code_mtb.member_type='REFEREE') tdbk_referral_code_mtb on  tdbk_referral_code_mtb.cust_id = acc.customer_id\n",
    "LEFT JOIN worktable_data_analysis.fraud_blacklist_clean fraud_blacklist_clean ON cast(fraud_blacklist_clean.customerid as string) = acc.customer_id\n",
    "LEFT JOIN worktable_data_analysis.reloan_customers_from_negative_locations negative_location ON cast(negative_location.customerid as string)= acc.customer_id\n",
    "LEFT JOIN `prj-prod-dataplatform.worktable_data_analysis.b_score_snapshot_contactability_20250807` contactability ON cast(contactability.customerid as string)= acc.customer_id --and contactability.ln_snapshot_date = acc.ln_snapshot_date\n",
    "--LEFT JOIN `worktable_data_analysis.b_score_snapshot_birthdate_info` backup on backup.customer_id = acc.customer_id\n",
    "LEFT JOIN `dl_customers_db_raw.tdbk_customer_mtb` cust on cust.cust_id = acc.customer_id\n",
    "LEFT JOIN cust_doc_data ON cust_doc_data.custid = acc.customer_id \n",
    "LEFT JOIN jira_data on jira_data.customer_id = acc.customer_id\n",
    "LEFT JOIN cust_email ON cust_email.cust_id = acc.customer_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
